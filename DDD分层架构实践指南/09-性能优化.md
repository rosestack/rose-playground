## 9. 性能优化

### 9.1 查询优化

#### 9.1.1 索引设计

在DDD分层架构中，合理的索引设计是查询性能优化的基础。我们需要根据业务查询模式和数据访问特点来设计索引策略。

##### 9.1.1.1 索引策略设计

**索引设计原则**

```java
/**
 * 数据库索引设计服务
 * <p>
 * 提供索引分析、设计和优化建议功能。
 *
 * @author 开发者
 * @since 1.0.0
 */
@Service
@Slf4j
public class IndexDesignService {

    private final DatabaseMetadataService metadataService;
    private final QueryStatisticsCollector statisticsCollector;
    private final IndexAnalyzer indexAnalyzer;

    public IndexDesignService(DatabaseMetadataService metadataService,
                             QueryStatisticsCollector statisticsCollector,
                             IndexAnalyzer indexAnalyzer) {
        this.metadataService = metadataService;
        this.statisticsCollector = statisticsCollector;
        this.indexAnalyzer = indexAnalyzer;
    }

    /**
     * 分析表的索引使用情况
     *
     * @param tableName 表名
     * @return 索引分析结果
     */
    public IndexAnalysisResult analyzeTableIndexes(String tableName) {
        // 获取表的所有索引
        List<IndexInfo> existingIndexes = metadataService.getTableIndexes(tableName);
        
        // 获取表的查询统计
        List<QueryStatistics> tableQueries = statisticsCollector.getQueriesByTable(tableName);
        
        // 分析索引使用效率
        List<IndexUsageAnalysis> usageAnalysis = analyzeIndexUsage(existingIndexes, tableQueries);
        
        // 识别缺失的索引
        List<MissingIndexSuggestion> missingSuggestions = identifyMissingIndexes(tableName, tableQueries);
        
        // 识别冗余索引
        List<RedundantIndexInfo> redundantIndexes = identifyRedundantIndexes(existingIndexes);
        
        return IndexAnalysisResult.builder()
                .tableName(tableName)
                .existingIndexes(existingIndexes)
                .usageAnalysis(usageAnalysis)
                .missingSuggestions(missingSuggestions)
                .redundantIndexes(redundantIndexes)
                .analysisTime(Instant.now())
                .build();
    }

    /**
     * 生成索引优化建议
     *
     * @param tableName 表名
     * @return 优化建议
     */
    public List<IndexOptimizationSuggestion> generateOptimizationSuggestions(String tableName) {
        IndexAnalysisResult analysis = analyzeTableIndexes(tableName);
        List<IndexOptimizationSuggestion> suggestions = new ArrayList<>();
        
        // 建议创建缺失的索引
        for (MissingIndexSuggestion missing : analysis.getMissingSuggestions()) {
            suggestions.add(IndexOptimizationSuggestion.builder()
                    .type(OptimizationType.CREATE_INDEX)
                    .priority(calculatePriority(missing))
                    .description(String.format("为字段 %s 创建索引", missing.getColumns()))
                    .sqlStatement(generateCreateIndexSql(tableName, missing))
                    .estimatedImpact(missing.getEstimatedImpact())
                    .build());
        }
        
        // 建议删除冗余索引
        for (RedundantIndexInfo redundant : analysis.getRedundantIndexes()) {
            suggestions.add(IndexOptimizationSuggestion.builder()
                    .type(OptimizationType.DROP_INDEX)
                    .priority(Priority.LOW)
                    .description(String.format("删除冗余索引 %s", redundant.getIndexName()))
                    .sqlStatement(String.format("DROP INDEX %s", redundant.getIndexName()))
                    .estimatedImpact("减少存储空间和写入开销")
                    .build());
        }
        
        // 建议优化现有索引
        for (IndexUsageAnalysis usage : analysis.getUsageAnalysis()) {
            if (usage.getUsageRate() < 0.1) { // 使用率低于10%
                suggestions.add(IndexOptimizationSuggestion.builder()
                        .type(OptimizationType.OPTIMIZE_INDEX)
                        .priority(Priority.MEDIUM)
                        .description(String.format("索引 %s 使用率较低，考虑重新设计", usage.getIndexName()))
                        .estimatedImpact("提高索引效率")
                        .build());
            }
        }
        
        return suggestions.stream()
                .sorted(Comparator.comparing(IndexOptimizationSuggestion::getPriority))
                .collect(Collectors.toList());
    }

    /**
     * 识别缺失的索引
     *
     * @param tableName 表名
     * @param queries 查询统计
     * @return 缺失索引建议
     */
    private List<MissingIndexSuggestion> identifyMissingIndexes(String tableName, List<QueryStatistics> queries) {
        Map<String, MissingIndexCandidate> candidates = new HashMap<>();
        
        for (QueryStatistics query : queries) {
            // 解析查询中的WHERE条件
            List<String> whereColumns = extractWhereColumns(query.getSqlTemplate());
            
            for (String column : whereColumns) {
                if (!metadataService.hasIndex(tableName, column)) {
                    String key = String.join(",", Collections.singletonList(column));
                    candidates.compute(key, (k, existing) -> {
                        if (existing == null) {
                            return MissingIndexCandidate.builder()
                                    .columns(Collections.singletonList(column))
                                    .queryCount(1)
                                    .totalExecutionTime(query.getTotalExecutionTime())
                                    .build();
                        } else {
                            return existing.toBuilder()
                                    .queryCount(existing.getQueryCount() + 1)
                                    .totalExecutionTime(existing.getTotalExecutionTime() + query.getTotalExecutionTime())
                                    .build();
                        }
                    });
                }
            }
        }
        
        return candidates.values().stream()
                .filter(candidate -> candidate.getQueryCount() >= 5) // 至少被5个查询使用
                .map(this::convertToSuggestion)
                .collect(Collectors.toList());
    }

    /**
     * 生成创建索引的SQL语句
     *
     * @param tableName 表名
     * @param suggestion 索引建议
     * @return SQL语句
     */
    private String generateCreateIndexSql(String tableName, MissingIndexSuggestion suggestion) {
        String indexName = String.format("idx_%s_%s", tableName, 
                String.join("_", suggestion.getColumns()));
        String columns = String.join(", ", suggestion.getColumns());
        
        return String.format("CREATE INDEX %s ON %s (%s)", indexName, tableName, columns);
    }
}
```

**索引监控和维护**

```java
/**
 * 索引监控服务
 * <p>
 * 监控索引使用情况和性能指标。
 *
 * @author 开发者
 * @since 1.0.0
 */
@Service
@Slf4j
public class IndexMonitoringService {

    private final DataSource dataSource;
    private final MeterRegistry meterRegistry;
    private final ScheduledExecutorService scheduler;

    public IndexMonitoringService(DataSource dataSource, MeterRegistry meterRegistry) {
        this.dataSource = dataSource;
        this.meterRegistry = meterRegistry;
        this.scheduler = Executors.newScheduledThreadPool(1);
        
        // 定期收集索引统计信息
        scheduleIndexStatisticsCollection();
    }

    /**
     * 收集索引使用统计
     */
    @Scheduled(fixedRate = 300000) // 每5分钟执行一次
    public void collectIndexStatistics() {
        try (Connection connection = dataSource.getConnection()) {
            // MySQL索引使用统计查询
            String sql = """
                SELECT 
                    s.table_schema,
                    s.table_name,
                    s.index_name,
                    s.rows_examined,
                    s.rows_read,
                    s.avg_timer_wait / 1000000000 as avg_time_ms
                FROM performance_schema.table_io_waits_summary_by_index_usage s
                WHERE s.table_schema = DATABASE()
                AND s.count_read > 0
                ORDER BY s.avg_timer_wait DESC
                """;
                
            try (PreparedStatement stmt = connection.prepareStatement(sql);
                 ResultSet rs = stmt.executeQuery()) {
                
                while (rs.next()) {
                    String tableName = rs.getString("table_name");
                    String indexName = rs.getString("index_name");
                    long rowsExamined = rs.getLong("rows_examined");
                    long rowsRead = rs.getLong("rows_read");
                    double avgTimeMs = rs.getDouble("avg_time_ms");
                    
                    // 记录指标
                    recordIndexMetrics(tableName, indexName, rowsExamined, rowsRead, avgTimeMs);
                }
            }
        } catch (SQLException e) {
            log.error("收集索引统计信息失败", e);
        }
    }

    /**
     * 记录索引指标
     */
    private void recordIndexMetrics(String tableName, String indexName, 
                                  long rowsExamined, long rowsRead, double avgTimeMs) {
        // 索引效率指标
        double efficiency = rowsRead > 0 ? (double) rowsRead / rowsExamined : 0;
        
        Gauge.builder("index.efficiency")
                .tag("table", tableName)
                .tag("index", indexName)
                .register(meterRegistry, () -> efficiency);
                
        Gauge.builder("index.avg_time_ms")
                .tag("table", tableName)
                .tag("index", indexName)
                .register(meterRegistry, () -> avgTimeMs);
    }

    /**
     * 定期收集索引统计信息
     */
    private void scheduleIndexStatisticsCollection() {
        scheduler.scheduleAtFixedRate(
                this::collectIndexStatistics,
                0, 5, TimeUnit.MINUTES
        );
    }
}
```

##### 9.1.1.2 复合索引设计

**复合索引优化策略**

```java
/**
 * 复合索引分析器
 * <p>
 * 分析和优化复合索引设计。
 *
 * @author 开发者
 * @since 1.0.0
 */
@Component
@Slf4j
public class CompositeIndexAnalyzer {

    /**
     * 分析复合索引的最优列顺序
     *
     * @param tableName 表名
     * @param columns 索引列
     * @param queries 相关查询
     * @return 最优列顺序
     */
    public List<String> optimizeColumnOrder(String tableName, List<String> columns, 
                                          List<QueryPattern> queries) {
        // 计算每列的选择性
        Map<String, Double> selectivity = calculateColumnSelectivity(tableName, columns);
        
        // 分析查询模式中的列使用频率
        Map<String, Integer> usageFrequency = analyzeColumnUsageFrequency(columns, queries);
        
        // 分析等值查询 vs 范围查询
        Map<String, QueryType> queryTypes = analyzeQueryTypes(columns, queries);
        
        // 根据索引设计原则排序
        return columns.stream()
                .sorted((col1, col2) -> {
                    // 1. 等值查询列优先于范围查询列
                    QueryType type1 = queryTypes.getOrDefault(col1, QueryType.EQUALITY);
                    QueryType type2 = queryTypes.getOrDefault(col2, QueryType.EQUALITY);
                    
                    if (type1 != type2) {
                        return type1 == QueryType.EQUALITY ? -1 : 1;
                    }
                    
                    // 2. 使用频率高的列优先
                    int freq1 = usageFrequency.getOrDefault(col1, 0);
                    int freq2 = usageFrequency.getOrDefault(col2, 0);
                    
                    if (freq1 != freq2) {
                        return Integer.compare(freq2, freq1);
                    }
                    
                    // 3. 选择性高的列优先
                    double sel1 = selectivity.getOrDefault(col1, 0.0);
                    double sel2 = selectivity.getOrDefault(col2, 0.0);
                    
                    return Double.compare(sel2, sel1);
                })
                .collect(Collectors.toList());
    }

    /**
     * 计算列的选择性
     *
     * @param tableName 表名
     * @param columns 列名列表
     * @return 选择性映射
     */
    private Map<String, Double> calculateColumnSelectivity(String tableName, List<String> columns) {
        Map<String, Double> selectivity = new HashMap<>();
        
        try (Connection connection = dataSource.getConnection()) {
            for (String column : columns) {
                String sql = String.format(
                    "SELECT COUNT(DISTINCT %s) / COUNT(*) as selectivity FROM %s", 
                    column, tableName);
                    
                try (PreparedStatement stmt = connection.prepareStatement(sql);
                     ResultSet rs = stmt.executeQuery()) {
                    
                    if (rs.next()) {
                        selectivity.put(column, rs.getDouble("selectivity"));
                    }
                }
            }
        } catch (SQLException e) {
            log.error("计算列选择性失败", e);
        }
        
        return selectivity;
    }

    /**
     * 生成复合索引建议
     *
     * @param tableName 表名
     * @param queryPatterns 查询模式
     * @return 复合索引建议
     */
    public List<CompositeIndexSuggestion> generateCompositeIndexSuggestions(
            String tableName, List<QueryPattern> queryPatterns) {
        
        List<CompositeIndexSuggestion> suggestions = new ArrayList<>();
        
        // 按查询频率分组
        Map<Set<String>, List<QueryPattern>> columnGroups = queryPatterns.stream()
                .collect(Collectors.groupingBy(
                    pattern -> new HashSet<>(pattern.getWhereColumns())));
        
        for (Map.Entry<Set<String>, List<QueryPattern>> entry : columnGroups.entrySet()) {
            Set<String> columns = entry.getKey();
            List<QueryPattern> patterns = entry.getValue();
            
            if (columns.size() > 1) { // 只考虑多列索引
                List<String> optimizedOrder = optimizeColumnOrder(tableName, 
                    new ArrayList<>(columns), patterns);
                
                double estimatedImpact = calculateEstimatedImpact(patterns);
                
                suggestions.add(CompositeIndexSuggestion.builder()
                        .tableName(tableName)
                        .columns(optimizedOrder)
                        .affectedQueries(patterns.size())
                        .estimatedImpact(estimatedImpact)
                        .priority(calculatePriority(estimatedImpact, patterns.size()))
                        .build());
            }
        }
        
        return suggestions.stream()
                .sorted(Comparator.comparing(CompositeIndexSuggestion::getPriority).reversed())
                .collect(Collectors.toList());
    }
}
```

#### 9.1.2 查询分析

在DDD分层架构中，查询分析是性能优化的关键环节。通过MyBatis-Plus的慢查询监控和SQL分析功能，我们可以识别性能瓶颈并进行针对性优化。

##### 9.1.2.1 MyBatis-Plus慢查询监控

**慢查询监控配置**

```java
/**
 * MyBatis-Plus性能监控配置
 * <p>
 * 提供慢查询监控、SQL分析和性能统计功能。
 * <p>
 * <h3>核心特性：</h3>
 * <ul>
 *   <li>慢查询检测和记录</li>
 *   <li>SQL执行时间统计</li>
 *   <li>查询性能分析</li>
 *   <li>自动性能报告</li>
 * </ul>
 *
 * @author 开发者
 * @since 1.0.0
 */
@Configuration
@EnableConfigurationProperties(QueryAnalysisProperties.class)
@Slf4j
public class QueryAnalysisConfig {

    private final QueryAnalysisProperties properties;

    public QueryAnalysisConfig(QueryAnalysisProperties properties) {
        this.properties = properties;
    }

    /**
     * 配置慢查询监控拦截器
     *
     * @return 慢查询监控拦截器
     */
    @Bean
    @ConditionalOnProperty(name = "query.analysis.slow-query.enabled", havingValue = "true", matchIfMissing = true)
    public SlowQueryInterceptor slowQueryInterceptor() {
        SlowQueryInterceptor interceptor = new SlowQueryInterceptor();
        interceptor.setSlowQueryThreshold(properties.getSlowQuery().getThreshold());
        interceptor.setLogSlowQuery(properties.getSlowQuery().isLogEnabled());
        interceptor.setMetricsEnabled(properties.getSlowQuery().isMetricsEnabled());
        return interceptor;
    }

    /**
     * 配置SQL性能分析拦截器
     *
     * @return SQL性能分析拦截器
     */
    @Bean
    @ConditionalOnProperty(name = "query.analysis.performance.enabled", havingValue = "true")
    public PerformanceInterceptor performanceInterceptor() {
        PerformanceInterceptor interceptor = new PerformanceInterceptor();
        interceptor.setMaxTime(properties.getPerformance().getMaxTime());
        interceptor.setFormat(properties.getPerformance().isFormat());
        interceptor.setWriteInLog(properties.getPerformance().isWriteInLog());
        return interceptor;
    }

    /**
     * 配置查询统计收集器
     *
     * @return 查询统计收集器
     */
    @Bean
    public QueryStatisticsCollector queryStatisticsCollector() {
        return new QueryStatisticsCollector(properties);
    }
}
```

**慢查询监控拦截器**

```java
/**
 * 慢查询监控拦截器
 * <p>
 * 监控SQL执行时间，记录慢查询并生成性能指标。
 *
 * @author 开发者
 * @since 1.0.0
 */
@Slf4j
@Component
public class SlowQueryInterceptor implements Interceptor {

    /** 慢查询阈值（毫秒） */
    private long slowQueryThreshold = 1000L;
    
    /** 是否记录慢查询日志 */
    private boolean logSlowQuery = true;
    
    /** 是否启用指标收集 */
    private boolean metricsEnabled = true;

    @Autowired
    private QueryStatisticsCollector statisticsCollector;

    @Autowired
    private MeterRegistry meterRegistry;

    @Override
    public Object intercept(Invocation invocation) throws Throwable {
        long startTime = System.currentTimeMillis();
        String sql = null;
        
        try {
            // 获取SQL语句
            sql = extractSql(invocation);
            
            // 执行查询
            Object result = invocation.proceed();
            
            // 计算执行时间
            long executionTime = System.currentTimeMillis() - startTime;
            
            // 处理查询结果
            handleQueryResult(sql, executionTime, result);
            
            return result;
        } catch (Exception e) {
            long executionTime = System.currentTimeMillis() - startTime;
            handleQueryError(sql, executionTime, e);
            throw e;
        }
    }

    /**
     * 处理查询结果
     *
     * @param sql 执行的SQL语句
     * @param executionTime 执行时间
     * @param result 查询结果
     */
    private void handleQueryResult(String sql, long executionTime, Object result) {
        // 记录慢查询
        if (executionTime > slowQueryThreshold) {
            recordSlowQuery(sql, executionTime);
        }
        
        // 收集统计信息
        if (metricsEnabled) {
            collectMetrics(sql, executionTime, true);
        }
        
        // 更新查询统计
        statisticsCollector.recordQuery(sql, executionTime, getResultCount(result));
    }

    /**
     * 记录慢查询
     *
     * @param sql SQL语句
     * @param executionTime 执行时间
     */
    private void recordSlowQuery(String sql, long executionTime) {
        if (logSlowQuery) {
            log.warn("慢查询检测 - 执行时间: {}ms, SQL: {}", executionTime, sql);
        }
        
        // 发送慢查询告警
        SlowQueryEvent event = SlowQueryEvent.builder()
                .sql(sql)
                .executionTime(executionTime)
                .threshold(slowQueryThreshold)
                .timestamp(Instant.now())
                .build();
                
        // 这里可以集成告警系统
        publishSlowQueryAlert(event);
    }

    /**
     * 收集性能指标
     *
     * @param sql SQL语句
     * @param executionTime 执行时间
     * @param success 是否成功
     */
    private void collectMetrics(String sql, long executionTime, boolean success) {
        String operation = extractOperation(sql);
        String table = extractTable(sql);
        
        // 记录执行时间
        Timer.Sample sample = Timer.start(meterRegistry);
        sample.stop(Timer.builder("sql.execution.time")
                .tag("operation", operation)
                .tag("table", table)
                .tag("success", String.valueOf(success))
                .register(meterRegistry));
        
        // 记录慢查询计数
        if (executionTime > slowQueryThreshold) {
            meterRegistry.counter("sql.slow.query.count",
                    "operation", operation,
                    "table", table)
                    .increment();
        }
    }

    /**
     * 提取SQL语句
     *
     * @param invocation 方法调用
     * @return SQL语句
     */
    private String extractSql(Invocation invocation) {
        Object target = invocation.getTarget();
        if (target instanceof StatementHandler) {
            StatementHandler handler = (StatementHandler) target;
            BoundSql boundSql = handler.getBoundSql();
            return boundSql.getSql();
        }
        return "Unknown SQL";
    }

    /**
     * 提取SQL操作类型
     *
     * @param sql SQL语句
     * @return 操作类型
     */
    private String extractOperation(String sql) {
        if (sql == null) return "unknown";
        String upperSql = sql.trim().toUpperCase();
        if (upperSql.startsWith("SELECT")) return "select";
        if (upperSql.startsWith("INSERT")) return "insert";
        if (upperSql.startsWith("UPDATE")) return "update";
        if (upperSql.startsWith("DELETE")) return "delete";
        return "other";
    }

    /**
     * 提取表名
     *
     * @param sql SQL语句
     * @return 表名
     */
    private String extractTable(String sql) {
        // 简化的表名提取逻辑
        // 实际项目中可以使用SQL解析器如JSqlParser
        if (sql == null) return "unknown";
        
        String upperSql = sql.trim().toUpperCase();
        Pattern pattern = Pattern.compile("(?:FROM|INTO|UPDATE)\\s+([\\w_]+)", Pattern.CASE_INSENSITIVE);
        Matcher matcher = pattern.matcher(upperSql);
        
        if (matcher.find()) {
            return matcher.group(1).toLowerCase();
        }
        return "unknown";
    }

    // Getter和Setter方法
    public void setSlowQueryThreshold(long slowQueryThreshold) {
        this.slowQueryThreshold = slowQueryThreshold;
    }

    public void setLogSlowQuery(boolean logSlowQuery) {
        this.logSlowQuery = logSlowQuery;
    }

    public void setMetricsEnabled(boolean metricsEnabled) {
        this.metricsEnabled = metricsEnabled;
    }
}
```

**查询统计收集器**

```java
/**
 * 查询统计收集器
 * <p>
 * 收集和分析SQL查询的统计信息，提供性能分析数据。
 *
 * @author 开发者
 * @since 1.0.0
 */
@Component
@Slf4j
public class QueryStatisticsCollector {

    private final QueryAnalysisProperties properties;
    private final ConcurrentHashMap<String, QueryStatistics> statisticsMap;
    private final ScheduledExecutorService scheduler;

    public QueryStatisticsCollector(QueryAnalysisProperties properties) {
        this.properties = properties;
        this.statisticsMap = new ConcurrentHashMap<>();
        this.scheduler = Executors.newScheduledThreadPool(2);
        
        // 定期生成统计报告
        if (properties.getReport().isEnabled()) {
            scheduleReportGeneration();
        }
    }

    /**
     * 记录查询执行信息
     *
     * @param sql SQL语句
     * @param executionTime 执行时间
     * @param resultCount 结果数量
     */
    public void recordQuery(String sql, long executionTime, int resultCount) {
        String sqlHash = generateSqlHash(sql);
        
        statisticsMap.compute(sqlHash, (key, existing) -> {
            if (existing == null) {
                return QueryStatistics.builder()
                        .sqlTemplate(normalizeSql(sql))
                        .executionCount(1L)
                        .totalExecutionTime(executionTime)
                        .minExecutionTime(executionTime)
                        .maxExecutionTime(executionTime)
                        .totalResultCount(resultCount)
                        .firstExecutionTime(Instant.now())
                        .lastExecutionTime(Instant.now())
                        .build();
            } else {
                return existing.toBuilder()
                        .executionCount(existing.getExecutionCount() + 1)
                        .totalExecutionTime(existing.getTotalExecutionTime() + executionTime)
                        .minExecutionTime(Math.min(existing.getMinExecutionTime(), executionTime))
                        .maxExecutionTime(Math.max(existing.getMaxExecutionTime(), executionTime))
                        .totalResultCount(existing.getTotalResultCount() + resultCount)
                        .lastExecutionTime(Instant.now())
                        .build();
            }
        });
    }

    /**
     * 获取查询统计信息
     *
     * @return 查询统计信息列表
     */
    public List<QueryStatistics> getStatistics() {
        return new ArrayList<>(statisticsMap.values());
    }

    /**
     * 获取慢查询统计
     *
     * @param threshold 慢查询阈值
     * @return 慢查询统计列表
     */
    public List<QueryStatistics> getSlowQueries(long threshold) {
        return statisticsMap.values().stream()
                .filter(stats -> stats.getAverageExecutionTime() > threshold)
                .sorted((a, b) -> Long.compare(b.getAverageExecutionTime(), a.getAverageExecutionTime()))
                .collect(Collectors.toList());
    }

    /**
     * 获取热点查询统计
     *
     * @param limit 返回数量限制
     * @return 热点查询统计列表
     */
    public List<QueryStatistics> getHotQueries(int limit) {
        return statisticsMap.values().stream()
                .sorted((a, b) -> Long.compare(b.getExecutionCount(), a.getExecutionCount()))
                .limit(limit)
                .collect(Collectors.toList());
    }

    /**
     * 清理过期统计数据
     */
    public void cleanupExpiredStatistics() {
        Instant cutoff = Instant.now().minus(properties.getStatistics().getRetentionPeriod());
        
        statisticsMap.entrySet().removeIf(entry -> 
                entry.getValue().getLastExecutionTime().isBefore(cutoff));
                
        log.info("清理过期查询统计数据，当前统计条目数: {}", statisticsMap.size());
    }

    /**
     * 生成SQL哈希值
     *
     * @param sql SQL语句
     * @return 哈希值
     */
    private String generateSqlHash(String sql) {
        String normalized = normalizeSql(sql);
        return DigestUtils.md5DigestAsHex(normalized.getBytes(StandardCharsets.UTF_8));
    }

    /**
     * 规范化SQL语句（移除参数值）
     *
     * @param sql 原始SQL
     * @return 规范化后的SQL
     */
    private String normalizeSql(String sql) {
        if (sql == null) return "";
        
        // 移除多余空格
        String normalized = sql.replaceAll("\\s+", " ").trim();
        
        // 替换参数占位符
        normalized = normalized.replaceAll("'[^']*'", "?");
        normalized = normalized.replaceAll("\\b\\d+\\b", "?");
        
        return normalized;
    }

    /**
     * 定期生成统计报告
     */
    private void scheduleReportGeneration() {
        scheduler.scheduleAtFixedRate(
                this::generatePerformanceReport,
                properties.getReport().getInitialDelay().toMinutes(),
                properties.getReport().getInterval().toMinutes(),
                TimeUnit.MINUTES
        );
        
        scheduler.scheduleAtFixedRate(
                this::cleanupExpiredStatistics,
                1, 1, TimeUnit.HOURS
        );
    }

    /**
     * 生成性能报告
     */
    private void generatePerformanceReport() {
        try {
            QueryPerformanceReport report = QueryPerformanceReport.builder()
                    .reportTime(Instant.now())
                    .totalQueries(statisticsMap.size())
                    .slowQueries(getSlowQueries(properties.getSlowQuery().getThreshold()))
                    .hotQueries(getHotQueries(properties.getReport().getTopQueriesLimit()))
                    .averageExecutionTime(calculateAverageExecutionTime())
                    .build();
                    
            log.info("查询性能报告: {}", report);
            
            // 这里可以将报告发送到监控系统或保存到文件
            publishPerformanceReport(report);
            
        } catch (Exception e) {
            log.error("生成查询性能报告失败", e);
        }
    }

    /**
     * 计算平均执行时间
     *
     * @return 平均执行时间
     */
    private double calculateAverageExecutionTime() {
        return statisticsMap.values().stream()
                .mapToLong(QueryStatistics::getAverageExecutionTime)
                .average()
                .orElse(0.0);
    }
}
```

##### 9.1.2.2 SQL分析和优化

**SQL分析服务**

```java
/**
 * SQL分析服务
 * <p>
 * 提供SQL语句的深度分析和优化建议。
 *
 * @author 开发者
 * @since 1.0.0
 */
@Service
@Slf4j
public class SqlAnalysisService {

    private final QueryStatisticsCollector statisticsCollector;
    private final DatabaseMetadataService metadataService;
    private final SqlOptimizationRuleEngine ruleEngine;

    public SqlAnalysisService(QueryStatisticsCollector statisticsCollector,
                             DatabaseMetadataService metadataService,
                             SqlOptimizationRuleEngine ruleEngine) {
        this.statisticsCollector = statisticsCollector;
        this.metadataService = metadataService;
        this.ruleEngine = ruleEngine;
    }

    /**
     * 分析SQL性能
     *
     * @param sql SQL语句
     * @return SQL分析结果
     */
    public SqlAnalysisResult analyzeSql(String sql) {
        try {
            // 解析SQL语句
            Statement statement = CCJSqlParserUtil.parse(sql);
            
            // 执行各种分析
            List<SqlIssue> issues = new ArrayList<>();
            issues.addAll(analyzeSelectStatement(statement));
            issues.addAll(analyzeIndexUsage(statement));
            issues.addAll(analyzeJoinConditions(statement));
            issues.addAll(analyzeWhereConditions(statement));
            
            // 生成优化建议
            List<OptimizationSuggestion> suggestions = ruleEngine.generateSuggestions(statement, issues);
            
            // 计算复杂度评分
            int complexityScore = calculateComplexityScore(statement);
            
            return SqlAnalysisResult.builder()
                    .sql(sql)
                    .statement(statement)
                    .issues(issues)
                    .suggestions(suggestions)
                    .complexityScore(complexityScore)
                    .analysisTime(Instant.now())
                    .build();
                    
        } catch (Exception e) {
            log.error("SQL分析失败: {}", sql, e);
            return SqlAnalysisResult.builder()
                    .sql(sql)
                    .error("SQL解析失败: " + e.getMessage())
                    .analysisTime(Instant.now())
                    .build();
        }
    }

    /**
     * 分析SELECT语句
     *
     * @param statement SQL语句对象
     * @return 发现的问题列表
     */
    private List<SqlIssue> analyzeSelectStatement(Statement statement) {
        List<SqlIssue> issues = new ArrayList<>();
        
        if (statement instanceof Select) {
            Select select = (Select) statement;
            PlainSelect plainSelect = (PlainSelect) select.getSelectBody();
            
            // 检查SELECT *
            if (hasSelectAll(plainSelect)) {
                issues.add(SqlIssue.builder()
                        .type(SqlIssueType.SELECT_ALL)
                        .severity(IssueSeverity.MEDIUM)
                        .message("使用SELECT *可能影响性能，建议明确指定需要的字段")
                        .suggestion("将SELECT *替换为具体的字段列表")
                        .build());
            }
            
            // 检查LIMIT子句
            if (plainSelect.getLimit() == null) {
                issues.add(SqlIssue.builder()
                        .type(SqlIssueType.MISSING_LIMIT)
                        .severity(IssueSeverity.LOW)
                        .message("缺少LIMIT子句，可能返回大量数据")
                        .suggestion("添加适当的LIMIT子句限制返回结果数量")
                        .build());
            }
            
            // 检查子查询
            if (hasSubqueries(plainSelect)) {
                issues.add(SqlIssue.builder()
                        .type(SqlIssueType.COMPLEX_SUBQUERY)
                        .severity(IssueSeverity.MEDIUM)
                        .message("复杂子查询可能影响性能")
                        .suggestion("考虑使用JOIN替换子查询或优化子查询逻辑")
                        .build());
            }
        }
        
        return issues;
    }

    /**
     * 分析索引使用情况
     *
     * @param statement SQL语句对象
     * @return 发现的问题列表
     */
    private List<SqlIssue> analyzeIndexUsage(Statement statement) {
        List<SqlIssue> issues = new ArrayList<>();
        
        if (statement instanceof Select) {
            Select select = (Select) statement;
            PlainSelect plainSelect = (PlainSelect) select.getSelectBody();
            
            // 获取涉及的表
            List<String> tables = extractTables(plainSelect);
            
            // 检查WHERE条件中的字段是否有索引
            if (plainSelect.getWhere() != null) {
                List<String> whereColumns = extractWhereColumns(plainSelect.getWhere());
                
                for (String table : tables) {
                    for (String column : whereColumns) {
                        if (!metadataService.hasIndex(table, column)) {
                            issues.add(SqlIssue.builder()
                                    .type(SqlIssueType.MISSING_INDEX)
                                    .severity(IssueSeverity.HIGH)
                                    .message(String.format("表%s的字段%s缺少索引", table, column))
                                    .suggestion(String.format("为表%s的字段%s创建索引", table, column))
                                    .build());
                        }
                    }
                }
            }
        }
        
        return issues;
    }

    /**
     * 生成查询优化报告
     *
     * @return 优化报告
     */
    public QueryOptimizationReport generateOptimizationReport() {
        List<QueryStatistics> slowQueries = statisticsCollector.getSlowQueries(1000L);
        List<QueryStatistics> hotQueries = statisticsCollector.getHotQueries(20);
        
        List<SqlAnalysisResult> slowQueryAnalysis = slowQueries.stream()
                .map(stats -> analyzeSql(stats.getSqlTemplate()))
                .collect(Collectors.toList());
                
        List<SqlAnalysisResult> hotQueryAnalysis = hotQueries.stream()
                .map(stats -> analyzeSql(stats.getSqlTemplate()))
                .collect(Collectors.toList());
        
        return QueryOptimizationReport.builder()
                .reportTime(Instant.now())
                .slowQueryCount(slowQueries.size())
                .hotQueryCount(hotQueries.size())
                .slowQueryAnalysis(slowQueryAnalysis)
                .hotQueryAnalysis(hotQueryAnalysis)
                .overallRecommendations(generateOverallRecommendations(slowQueryAnalysis, hotQueryAnalysis))
                .build();
    }

    /**
     * 生成整体优化建议
     *
     * @param slowQueryAnalysis 慢查询分析结果
     * @param hotQueryAnalysis 热点查询分析结果
     * @return 整体优化建议列表
     */
    private List<String> generateOverallRecommendations(List<SqlAnalysisResult> slowQueryAnalysis,
                                                       List<SqlAnalysisResult> hotQueryAnalysis) {
        List<String> recommendations = new ArrayList<>();
        
        // 统计问题类型
        Map<SqlIssueType, Long> issueTypeCount = Stream.concat(
                slowQueryAnalysis.stream(),
                hotQueryAnalysis.stream())
                .flatMap(result -> result.getIssues().stream())
                .collect(Collectors.groupingBy(SqlIssue::getType, Collectors.counting()));
        
        // 生成针对性建议
        if (issueTypeCount.getOrDefault(SqlIssueType.MISSING_INDEX, 0L) > 5) {
            recommendations.add("检测到多个缺少索引的查询，建议进行索引优化");
        }
        
        if (issueTypeCount.getOrDefault(SqlIssueType.SELECT_ALL, 0L) > 3) {
            recommendations.add("多个查询使用SELECT *，建议明确指定需要的字段");
        }
        
        if (issueTypeCount.getOrDefault(SqlIssueType.COMPLEX_SUBQUERY, 0L) > 2) {
            recommendations.add("存在复杂子查询，建议考虑使用JOIN优化");
        }
        
        return recommendations;
    }
}
```

**配置属性类**

```java
/**
 * 查询分析配置属性
 *
 * @author 开发者
 * @since 1.0.0
 */
@ConfigurationProperties(prefix = "query.analysis")
@Data
public class QueryAnalysisProperties {

    /** 慢查询配置 */
    private SlowQuery slowQuery = new SlowQuery();
    
    /** 性能分析配置 */
    private Performance performance = new Performance();
    
    /** 统计配置 */
    private Statistics statistics = new Statistics();
    
    /** 报告配置 */
    private Report report = new Report();

    @Data
    public static class SlowQuery {
        /** 是否启用慢查询监控 */
        private boolean enabled = true;
        
        /** 慢查询阈值（毫秒） */
        private long threshold = 1000L;
        
        /** 是否记录日志 */
        private boolean logEnabled = true;
        
        /** 是否启用指标收集 */
        private boolean metricsEnabled = true;
    }

    @Data
    public static class Performance {
        /** 是否启用性能分析 */
        private boolean enabled = false;
        
        /** 最大执行时间（毫秒） */
        private long maxTime = 5000L;
        
        /** 是否格式化SQL */
        private boolean format = true;
        
        /** 是否写入日志 */
        private boolean writeInLog = true;
    }

    @Data
    public static class Statistics {
        /** 统计数据保留期 */
        private Duration retentionPeriod = Duration.ofDays(7);
        
        /** 最大统计条目数 */
        private int maxEntries = 10000;
    }

    @Data
    public static class Report {
        /** 是否启用报告生成 */
        private boolean enabled = true;
        
        /** 初始延迟 */
        private Duration initialDelay = Duration.ofMinutes(10);
        
        /** 生成间隔 */
        private Duration interval = Duration.ofHours(1);
        
        /** 热点查询数量限制 */
        private int topQueriesLimit = 20;
    }
}
```

**应用配置示例**

```yaml
# application.yml
query:
  analysis:
    slow-query:
      enabled: true
      threshold: 1000
      log-enabled: true
      metrics-enabled: true
    performance:
      enabled: false
      max-time: 5000
      format: true
      write-in-log: true
    statistics:
      retention-period: P7D
      max-entries: 10000
    report:
      enabled: true
      initial-delay: PT10M
      interval: PT1H
      top-queries-limit: 20

# MyBatis-Plus配置
mybatis-plus:
  configuration:
    log-impl: org.apache.ibatis.logging.slf4j.Slf4jImpl
  global-config:
    db-config:
      logic-delete-field: deleted
      logic-delete-value: 1
      logic-not-delete-value: 0

# 监控配置
management:
  endpoints:
    web:
      exposure:
        include: health,info,metrics,prometheus
  metrics:
    export:
      prometheus:
        enabled: true
```

通过以上配置，我们实现了完整的MyBatis-Plus慢查询监控和SQL分析功能，包括：

1. **慢查询监控**：自动检测和记录执行时间超过阈值的查询
2. **性能指标收集**：收集查询执行时间、频次等指标
3. **SQL分析**：深度分析SQL语句，发现潜在问题
4. **优化建议**：基于分析结果提供具体的优化建议
5. **定期报告**：自动生成性能分析报告

这些功能帮助开发团队及时发现和解决查询性能问题，持续优化系统性能。

#### 9.1.3 分页优化

在DDD分层架构中，分页查询是常见的性能瓶颈。我们需要针对不同场景采用合适的分页策略。

##### 9.1.3.1 游标分页实现

**游标分页服务**

```java
/**
 * 游标分页服务
 * <p>
 * 提供高性能的游标分页实现，适用于大数据量场景。
 *
 * @author 开发者
 * @since 1.0.0
 */
@Service
@Slf4j
public class CursorPaginationService {

    private final UserRepository userRepository;
    private final OrderRepository orderRepository;

    public CursorPaginationService(UserRepository userRepository, OrderRepository orderRepository) {
        this.userRepository = userRepository;
        this.orderRepository = orderRepository;
    }

    /**
     * 用户游标分页查询
     *
     * @param cursor 游标
     * @param limit 限制数量
     * @param direction 方向
     * @return 分页结果
     */
    public CursorPageResult<UserDto> getUsersByCursor(String cursor, int limit, Direction direction) {
        // 解析游标
        CursorInfo cursorInfo = parseCursor(cursor);
        
        // 构建查询条件
        LambdaQueryWrapper<User> queryWrapper = new LambdaQueryWrapper<>();
        
        if (cursorInfo != null) {
            if (direction == Direction.FORWARD) {
                queryWrapper.gt(User::getId, cursorInfo.getId());
                queryWrapper.orderByAsc(User::getId);
            } else {
                queryWrapper.lt(User::getId, cursorInfo.getId());
                queryWrapper.orderByDesc(User::getId);
            }
        } else {
            queryWrapper.orderByAsc(User::getId);
        }
        
        // 查询数据（多查一条用于判断是否有下一页）
        List<User> users = userRepository.list(queryWrapper.last("LIMIT " + (limit + 1)));
        
        // 处理结果
        boolean hasNext = users.size() > limit;
        if (hasNext) {
            users = users.subList(0, limit);
        }
        
        // 生成下一页游标
        String nextCursor = null;
        if (hasNext && !users.isEmpty()) {
            User lastUser = users.get(users.size() - 1);
            nextCursor = generateCursor(lastUser.getId(), lastUser.getCreatedAt());
        }
        
        // 转换为DTO
        List<UserDto> userDtos = users.stream()
                .map(this::convertToDto)
                .collect(Collectors.toList());
        
        return CursorPageResult.<UserDto>builder()
                .data(userDtos)
                .nextCursor(nextCursor)
                .hasNext(hasNext)
                .limit(limit)
                .build();
    }

    /**
     * 复合游标分页（支持多字段排序）
     *
     * @param cursor 游标
     * @param limit 限制数量
     * @param sortFields 排序字段
     * @return 分页结果
     */
    public CursorPageResult<OrderDto> getOrdersByCompositeCursor(String cursor, int limit, 
                                                               List<SortField> sortFields) {
        CompositeCursorInfo cursorInfo = parseCompositeCursor(cursor);
        
        LambdaQueryWrapper<Order> queryWrapper = new LambdaQueryWrapper<>();
        
        if (cursorInfo != null) {
            // 构建复合条件
            buildCompositeCondition(queryWrapper, cursorInfo, sortFields);
        }
        
        // 添加排序
        addSortConditions(queryWrapper, sortFields);
        
        List<Order> orders = orderRepository.list(queryWrapper.last("LIMIT " + (limit + 1)));
        
        boolean hasNext = orders.size() > limit;
        if (hasNext) {
            orders = orders.subList(0, limit);
        }
        
        String nextCursor = null;
        if (hasNext && !orders.isEmpty()) {
            Order lastOrder = orders.get(orders.size() - 1);
            nextCursor = generateCompositeCursor(lastOrder, sortFields);
        }
        
        List<OrderDto> orderDtos = orders.stream()
                .map(this::convertToOrderDto)
                .collect(Collectors.toList());
        
        return CursorPageResult.<OrderDto>builder()
                .data(orderDtos)
                .nextCursor(nextCursor)
                .hasNext(hasNext)
                .limit(limit)
                .build();
    }

    /**
     * 解析游标信息
     *
     * @param cursor 游标字符串
     * @return 游标信息
     */
    private CursorInfo parseCursor(String cursor) {
        if (StringUtils.isBlank(cursor)) {
            return null;
        }
        
        try {
            String decoded = new String(Base64.getDecoder().decode(cursor), StandardCharsets.UTF_8);
            String[] parts = decoded.split(":");
            
            if (parts.length >= 2) {
                return CursorInfo.builder()
                        .id(Long.parseLong(parts[0]))
                        .timestamp(Instant.parse(parts[1]))
                        .build();
            }
        } catch (Exception e) {
            log.warn("解析游标失败: {}", cursor, e);
        }
        
        return null;
    }

    /**
     * 生成游标
     *
     * @param id ID
     * @param timestamp 时间戳
     * @return 游标字符串
     */
    private String generateCursor(Long id, Instant timestamp) {
        String cursorData = id + ":" + timestamp.toString();
        return Base64.getEncoder().encodeToString(cursorData.getBytes(StandardCharsets.UTF_8));
    }

    /**
     * 构建复合条件
     *
     * @param queryWrapper 查询包装器
     * @param cursorInfo 游标信息
     * @param sortFields 排序字段
     */
    private void buildCompositeCondition(LambdaQueryWrapper<Order> queryWrapper,
                                       CompositeCursorInfo cursorInfo,
                                       List<SortField> sortFields) {
        // 实现复合游标的条件构建逻辑
        // 这里需要根据具体的排序字段来构建WHERE条件
        for (int i = 0; i < sortFields.size(); i++) {
            SortField field = sortFields.get(i);
            Object value = cursorInfo.getValues().get(i);
            
            if (i == sortFields.size() - 1) {
                // 最后一个字段使用大于条件
                if (field.getDirection() == Direction.ASC) {
                    queryWrapper.gt(getColumnFunction(field.getFieldName()), value);
                } else {
                    queryWrapper.lt(getColumnFunction(field.getFieldName()), value);
                }
            } else {
                // 前面的字段使用等于条件或者构建复合条件
                queryWrapper.eq(getColumnFunction(field.getFieldName()), value);
            }
        }
    }
}
```

##### 9.1.3.2 深度分页优化

**深度分页优化策略**

```java
/**
 * 深度分页优化服务
 * <p>
 * 针对深度分页场景的性能优化。
 *
 * @author 开发者
 * @since 1.0.0
 */
@Service
@Slf4j
public class DeepPaginationOptimizer {

    private final RedisTemplate<String, Object> redisTemplate;
    private final UserRepository userRepository;

    public DeepPaginationOptimizer(RedisTemplate<String, Object> redisTemplate,
                                 UserRepository userRepository) {
        this.redisTemplate = redisTemplate;
        this.userRepository = userRepository;
    }

    /**
     * 延迟关联分页查询
     * 先查询ID，再关联查询详细信息
     *
     * @param pageNum 页码
     * @param pageSize 页大小
     * @param queryCondition 查询条件
     * @return 分页结果
     */
    public PageResult<UserDto> getPageByDelayedJoin(int pageNum, int pageSize, 
                                                   UserQueryCondition queryCondition) {
        // 第一步：只查询ID和排序字段
        LambdaQueryWrapper<User> idQueryWrapper = new LambdaQueryWrapper<>();
        buildQueryCondition(idQueryWrapper, queryCondition);
        idQueryWrapper.select(User::getId, User::getCreatedAt);
        idQueryWrapper.orderByDesc(User::getCreatedAt);
        
        // 计算偏移量
        int offset = (pageNum - 1) * pageSize;
        
        // 使用子查询优化深度分页
        String subQuery = String.format(
            "SELECT id FROM user WHERE %s ORDER BY created_at DESC LIMIT %d, %d",
            buildWhereClause(queryCondition), offset, pageSize
        );
        
        // 第二步：根据ID查询完整信息
        List<Long> userIds = userRepository.selectIdsBySubQuery(subQuery);
        
        if (userIds.isEmpty()) {
            return PageResult.<UserDto>builder()
                    .data(Collections.emptyList())
                    .total(0L)
                    .pageNum(pageNum)
                    .pageSize(pageSize)
                    .build();
        }
        
        // 查询完整用户信息
        LambdaQueryWrapper<User> detailQueryWrapper = new LambdaQueryWrapper<>();
        detailQueryWrapper.in(User::getId, userIds);
        detailQueryWrapper.orderByDesc(User::getCreatedAt);
        
        List<User> users = userRepository.list(detailQueryWrapper);
        
        // 查询总数（使用缓存）
        Long total = getTotalCountWithCache(queryCondition);
        
        List<UserDto> userDtos = users.stream()
                .map(this::convertToDto)
                .collect(Collectors.toList());
        
        return PageResult.<UserDto>builder()
                .data(userDtos)
                .total(total)
                .pageNum(pageNum)
                .pageSize(pageSize)
                .build();
    }

    /**
     * 缓存总数查询
     *
     * @param queryCondition 查询条件
     * @return 总数
     */
    private Long getTotalCountWithCache(UserQueryCondition queryCondition) {
        String cacheKey = "user:count:" + queryCondition.getCacheKey();
        
        Long cachedCount = (Long) redisTemplate.opsForValue().get(cacheKey);
        if (cachedCount != null) {
            return cachedCount;
        }
        
        // 查询总数
        LambdaQueryWrapper<User> countWrapper = new LambdaQueryWrapper<>();
        buildQueryCondition(countWrapper, queryCondition);
        Long total = userRepository.selectCount(countWrapper);
        
        // 缓存5分钟
        redisTemplate.opsForValue().set(cacheKey, total, Duration.ofMinutes(5));
        
        return total;
    }

    /**
     * 滚动分页查询
     * 适用于实时数据流场景
     *
     * @param lastId 最后一条记录的ID
     * @param limit 限制数量
     * @return 查询结果
     */
    public List<UserDto> getPageByScroll(Long lastId, int limit) {
        LambdaQueryWrapper<User> queryWrapper = new LambdaQueryWrapper<>();
        
        if (lastId != null) {
            queryWrapper.gt(User::getId, lastId);
        }
        
        queryWrapper.orderByAsc(User::getId);
        queryWrapper.last("LIMIT " + limit);
        
        List<User> users = userRepository.list(queryWrapper);
        
        return users.stream()
                .map(this::convertToDto)
                .collect(Collectors.toList());
    }
}
```

### 9.2 缓存优化

#### 9.2.1 多级缓存

在DDD分层架构中，多级缓存策略可以显著提升系统性能，减少数据库访问压力。

##### 9.2.1.1 缓存架构设计

**多级缓存管理器**

```java
/**
 * 多级缓存管理器
 * <p>
 * 实现L1（本地缓存）+ L2（分布式缓存）的多级缓存架构。
 *
 * @author 开发者
 * @since 1.0.0
 */
@Component
@Slf4j
public class MultiLevelCacheManager {

    private final Cache<String, Object> localCache;
    private final RedisTemplate<String, Object> redisTemplate;
    private final CacheMetricsCollector metricsCollector;
    private final CacheConfiguration cacheConfig;

    public MultiLevelCacheManager(RedisTemplate<String, Object> redisTemplate,
                                CacheMetricsCollector metricsCollector,
                                CacheConfiguration cacheConfig) {
        this.redisTemplate = redisTemplate;
        this.metricsCollector = metricsCollector;
        this.cacheConfig = cacheConfig;
        
        // 初始化本地缓存
        this.localCache = Caffeine.newBuilder()
                .maximumSize(cacheConfig.getLocalCacheMaxSize())
                .expireAfterWrite(cacheConfig.getLocalCacheExpiration())
                .recordStats()
                .removalListener(this::onLocalCacheRemoval)
                .build();
    }

    /**
     * 获取缓存值
     *
     * @param key 缓存键
     * @param type 值类型
     * @param loader 数据加载器
     * @return 缓存值
     */
    public <T> T get(String key, Class<T> type, Supplier<T> loader) {
        long startTime = System.currentTimeMillis();
        
        try {
            // L1缓存查询
            Object localValue = localCache.getIfPresent(key);
            if (localValue != null) {
                metricsCollector.recordCacheHit("L1", key);
                return type.cast(localValue);
            }
            
            // L2缓存查询
            Object redisValue = redisTemplate.opsForValue().get(key);
            if (redisValue != null) {
                metricsCollector.recordCacheHit("L2", key);
                // 回填L1缓存
                localCache.put(key, redisValue);
                return type.cast(redisValue);
            }
            
            // 缓存未命中，加载数据
            metricsCollector.recordCacheMiss(key);
            T value = loader.get();
            
            if (value != null) {
                // 写入多级缓存
                put(key, value);
            }
            
            return value;
            
        } finally {
            long duration = System.currentTimeMillis() - startTime;
            metricsCollector.recordCacheAccessTime(key, duration);
        }
    }

    /**
     * 设置缓存值
     *
     * @param key 缓存键
     * @param value 缓存值
     */
    public void put(String key, Object value) {
        if (value == null) {
            return;
        }
        
        // 写入L1缓存
        localCache.put(key, value);
        
        // 写入L2缓存
        Duration expiration = getCacheExpiration(key);
        redisTemplate.opsForValue().set(key, value, expiration);
        
        metricsCollector.recordCacheWrite(key);
    }

    /**
     * 删除缓存
     *
     * @param key 缓存键
     */
    public void evict(String key) {
        // 删除L1缓存
        localCache.invalidate(key);
        
        // 删除L2缓存
        redisTemplate.delete(key);
        
        // 发送缓存失效通知
        publishCacheEvictionEvent(key);
        
        metricsCollector.recordCacheEviction(key);
    }

    /**
     * 批量删除缓存
     *
     * @param pattern 键模式
     */
    public void evictByPattern(String pattern) {
        // 删除L1缓存中匹配的键
        localCache.asMap().keySet().stream()
                .filter(key -> key.matches(pattern.replace("*", ".*")))
                .forEach(localCache::invalidate);
        
        // 删除L2缓存中匹配的键
        Set<String> keysToDelete = redisTemplate.keys(pattern);
        if (!keysToDelete.isEmpty()) {
            redisTemplate.delete(keysToDelete);
        }
        
        metricsCollector.recordBatchEviction(pattern, keysToDelete.size());
    }

    /**
     * 缓存预热
     *
     * @param preloadTasks 预加载任务
     */
    public void warmUp(List<CachePreloadTask> preloadTasks) {
        log.info("开始缓存预热，任务数量: {}", preloadTasks.size());
        
        CompletableFuture<Void>[] futures = preloadTasks.stream()
                .map(task -> CompletableFuture.runAsync(() -> {
                    try {
                        Object value = task.getLoader().get();
                        if (value != null) {
                            put(task.getKey(), value);
                        }
                    } catch (Exception e) {
                        log.error("缓存预热失败: {}", task.getKey(), e);
                    }
                }))
                .toArray(CompletableFuture[]::new);
        
        CompletableFuture.allOf(futures).join();
        log.info("缓存预热完成");
    }

    /**
     * 获取缓存统计信息
     *
     * @return 统计信息
     */
    public CacheStatistics getStatistics() {
        CacheStats localStats = localCache.stats();
        
        return CacheStatistics.builder()
                .localCacheSize(localCache.estimatedSize())
                .localHitRate(localStats.hitRate())
                .localMissRate(localStats.missRate())
                .localEvictionCount(localStats.evictionCount())
                .totalHits(metricsCollector.getTotalHits())
                .totalMisses(metricsCollector.getTotalMisses())
                .averageAccessTime(metricsCollector.getAverageAccessTime())
                .build();
    }

    /**
     * 本地缓存移除监听器
     *
     * @param key 键
     * @param value 值
     * @param cause 移除原因
     */
    private void onLocalCacheRemoval(String key, Object value, RemovalCause cause) {
        log.debug("本地缓存移除: key={}, cause={}", key, cause);
        metricsCollector.recordLocalCacheRemoval(key, cause.toString());
    }

    /**
     * 发送缓存失效事件
     *
     * @param key 缓存键
     */
    private void publishCacheEvictionEvent(String key) {
        // 通过Redis发布/订阅通知其他节点
        CacheEvictionEvent event = CacheEvictionEvent.builder()
                .key(key)
                .timestamp(Instant.now())
                .nodeId(getNodeId())
                .build();
                
        redisTemplate.convertAndSend("cache:eviction", event);
    }

    /**
     * 获取缓存过期时间
     *
     * @param key 缓存键
     * @return 过期时间
     */
    private Duration getCacheExpiration(String key) {
        // 根据键的类型返回不同的过期时间
        if (key.startsWith("user:")) {
            return cacheConfig.getUserCacheExpiration();
        } else if (key.startsWith("order:")) {
            return cacheConfig.getOrderCacheExpiration();
        } else {
            return cacheConfig.getDefaultCacheExpiration();
        }
    }
}
```

##### 9.2.1.2 缓存同步策略

**缓存同步服务**

```java
/**
 * 缓存同步服务
 * <p>
 * 处理分布式环境下的缓存一致性问题。
 *
 * @author 开发者
 * @since 1.0.0
 */
@Service
@Slf4j
public class CacheSynchronizationService {

    private final RedisTemplate<String, Object> redisTemplate;
    private final MultiLevelCacheManager cacheManager;
    private final ApplicationEventPublisher eventPublisher;

    public CacheSynchronizationService(RedisTemplate<String, Object> redisTemplate,
                                     MultiLevelCacheManager cacheManager,
                                     ApplicationEventPublisher eventPublisher) {
        this.redisTemplate = redisTemplate;
        this.cacheManager = cacheManager;
        this.eventPublisher = eventPublisher;
    }

    /**
     * 监听缓存失效事件
     *
     * @param event 缓存失效事件
     */
    @EventListener
    public void handleCacheEvictionEvent(CacheEvictionEvent event) {
        // 只处理来自其他节点的失效事件
        if (!event.getNodeId().equals(getNodeId())) {
            log.debug("处理来自节点 {} 的缓存失效事件: {}", event.getNodeId(), event.getKey());
            cacheManager.evictLocal(event.getKey());
        }
    }

    /**
     * 数据更新时的缓存同步
     *
     * @param entity 实体对象
     * @param operation 操作类型
     */
    public void syncCacheOnDataChange(Object entity, CacheOperation operation) {
        String entityType = entity.getClass().getSimpleName().toLowerCase();
        
        switch (operation) {
            case UPDATE:
                handleUpdateOperation(entity, entityType);
                break;
            case DELETE:
                handleDeleteOperation(entity, entityType);
                break;
            case CREATE:
                handleCreateOperation(entity, entityType);
                break;
        }
    }

    /**
     * 处理更新操作
     *
     * @param entity 实体对象
     * @param entityType 实体类型
     */
    private void handleUpdateOperation(Object entity, String entityType) {
        // 获取实体ID
        Long entityId = getEntityId(entity);
        
        // 失效相关缓存
        List<String> cacheKeys = generateRelatedCacheKeys(entityType, entityId);
        for (String key : cacheKeys) {
            cacheManager.evict(key);
        }
        
        // 发布缓存更新事件
        CacheUpdateEvent updateEvent = CacheUpdateEvent.builder()
                .entityType(entityType)
                .entityId(entityId)
                .operation(CacheOperation.UPDATE)
                .timestamp(Instant.now())
                .build();
                
        eventPublisher.publishEvent(updateEvent);
    }

    /**
     * 处理删除操作
     *
     * @param entity 实体对象
     * @param entityType 实体类型
     */
    private void handleDeleteOperation(Object entity, String entityType) {
        Long entityId = getEntityId(entity);
        
        // 删除所有相关缓存
        String pattern = entityType + ":" + entityId + ":*";
        cacheManager.evictByPattern(pattern);
        
        // 删除实体缓存
        String entityKey = entityType + ":" + entityId;
        cacheManager.evict(entityKey);
    }

    /**
     * 生成相关缓存键
     *
     * @param entityType 实体类型
     * @param entityId 实体ID
     * @return 缓存键列表
     */
    private List<String> generateRelatedCacheKeys(String entityType, Long entityId) {
        List<String> keys = new ArrayList<>();
        
        // 实体缓存键
        keys.add(entityType + ":" + entityId);
        
        // 列表缓存键（根据实体类型生成）
        if ("user".equals(entityType)) {
            keys.add("user:list:*");
            keys.add("user:search:*");
        } else if ("order".equals(entityType)) {
            keys.add("order:list:*");
            keys.add("order:user:" + getUserIdFromOrder(entityId) + ":*");
        }
        
        return keys;
    }
}
```

#### 9.2.2 缓存预热

**缓存预热策略**

```java
/**
 * 缓存预热服务
 * <p>
 * 在系统启动或低峰期预加载热点数据到缓存。
 *
 * @author 开发者
 * @since 1.0.0
 */
@Service
@Slf4j
public class CacheWarmupService {

    private final MultiLevelCacheManager cacheManager;
    private final UserRepository userRepository;
    private final OrderRepository orderRepository;
    private final ProductRepository productRepository;
    private final ScheduledExecutorService scheduler;

    public CacheWarmupService(MultiLevelCacheManager cacheManager,
                            UserRepository userRepository,
                            OrderRepository orderRepository,
                            ProductRepository productRepository) {
        this.cacheManager = cacheManager;
        this.userRepository = userRepository;
        this.orderRepository = orderRepository;
        this.productRepository = productRepository;
        this.scheduler = Executors.newScheduledThreadPool(4);
    }

    /**
     * 系统启动时的缓存预热
     */
    @EventListener(ApplicationReadyEvent.class)
    public void warmupOnStartup() {
        log.info("开始系统启动缓存预热");
        
        CompletableFuture<Void> userWarmup = CompletableFuture.runAsync(this::warmupActiveUsers);
        CompletableFuture<Void> productWarmup = CompletableFuture.runAsync(this::warmupHotProducts);
        CompletableFuture<Void> configWarmup = CompletableFuture.runAsync(this::warmupSystemConfig);
        
        CompletableFuture.allOf(userWarmup, productWarmup, configWarmup)
                .thenRun(() -> log.info("系统启动缓存预热完成"))
                .exceptionally(throwable -> {
                    log.error("缓存预热失败", throwable);
                    return null;
                });
    }

    /**
     * 预热活跃用户数据
     */
    private void warmupActiveUsers() {
        log.info("开始预热活跃用户数据");
        
        // 查询最近30天活跃的用户
        Instant thirtyDaysAgo = Instant.now().minus(30, ChronoUnit.DAYS);
        LambdaQueryWrapper<User> queryWrapper = new LambdaQueryWrapper<>();
        queryWrapper.gt(User::getLastLoginTime, thirtyDaysAgo);
        queryWrapper.eq(User::getStatus, UserStatus.ACTIVE);
        queryWrapper.orderByDesc(User::getLastLoginTime);
        queryWrapper.last("LIMIT 1000"); // 限制预热数量
        
        List<User> activeUsers = userRepository.list(queryWrapper);
        
        for (User user : activeUsers) {
            try {
                String cacheKey = "user:" + user.getId();
                UserDto userDto = convertToUserDto(user);
                cacheManager.put(cacheKey, userDto);
                
                // 预热用户权限信息
                String permissionKey = "user:permissions:" + user.getId();
                List<String> permissions = getUserPermissions(user.getId());
                cacheManager.put(permissionKey, permissions);
                
            } catch (Exception e) {
                log.error("预热用户数据失败: userId={}", user.getId(), e);
            }
        }
        
        log.info("活跃用户数据预热完成，预热用户数: {}", activeUsers.size());
    }

    /**
     * 预热热门商品数据
     */
    private void warmupHotProducts() {
        log.info("开始预热热门商品数据");
        
        // 查询热门商品（根据销量排序）
        LambdaQueryWrapper<Product> queryWrapper = new LambdaQueryWrapper<>();
        queryWrapper.eq(Product::getStatus, ProductStatus.ACTIVE);
        queryWrapper.orderByDesc(Product::getSalesCount);
        queryWrapper.last("LIMIT 500");
        
        List<Product> hotProducts = productRepository.list(queryWrapper);
        
        for (Product product : hotProducts) {
            try {
                String cacheKey = "product:" + product.getId();
                ProductDto productDto = convertToProductDto(product);
                cacheManager.put(cacheKey, productDto);
                
                // 预热商品库存信息
                String stockKey = "product:stock:" + product.getId();
                Integer stock = getProductStock(product.getId());
                cacheManager.put(stockKey, stock);
                
            } catch (Exception e) {
                log.error("预热商品数据失败: productId={}", product.getId(), e);
            }
        }
        
        log.info("热门商品数据预热完成，预热商品数: {}", hotProducts.size());
    }

    /**
     * 预热系统配置数据
     */
    private void warmupSystemConfig() {
        log.info("开始预热系统配置数据");
        
        try {
            // 预热系统参数
            Map<String, String> systemParams = getSystemParameters();
            cacheManager.put("system:params", systemParams);
            
            // 预热字典数据
            Map<String, List<DictItem>> dictData = getDictionaryData();
            for (Map.Entry<String, List<DictItem>> entry : dictData.entrySet()) {
                String cacheKey = "dict:" + entry.getKey();
                cacheManager.put(cacheKey, entry.getValue());
            }
            
            // 预热菜单权限数据
            List<MenuPermission> menuPermissions = getMenuPermissions();
            cacheManager.put("menu:permissions", menuPermissions);
            
            log.info("系统配置数据预热完成");
            
        } catch (Exception e) {
            log.error("预热系统配置数据失败", e);
        }
    }

    /**
     * 定时缓存预热任务
     */
    @Scheduled(cron = "0 0 2 * * ?") // 每天凌晨2点执行
    public void scheduledWarmup() {
        log.info("开始定时缓存预热");
        
        // 预热昨日热点数据
        warmupYesterdayHotData();
        
        // 预热即将到期的缓存
        refreshExpiringCache();
        
        log.info("定时缓存预热完成");
    }

    /**
     * 预热昨日热点数据
     */
    private void warmupYesterdayHotData() {
        Instant yesterday = Instant.now().minus(1, ChronoUnit.DAYS);
        Instant dayBeforeYesterday = yesterday.minus(1, ChronoUnit.DAYS);
        
        // 查询昨日热门订单
        LambdaQueryWrapper<Order> orderQuery = new LambdaQueryWrapper<>();
        orderQuery.between(Order::getCreatedAt, dayBeforeYesterday, yesterday);
        orderQuery.orderByDesc(Order::getTotalAmount);
        orderQuery.last("LIMIT 100");
        
        List<Order> hotOrders = orderRepository.list(orderQuery);
        
        for (Order order : hotOrders) {
            String cacheKey = "order:" + order.getId();
            OrderDto orderDto = convertToOrderDto(order);
            cacheManager.put(cacheKey, orderDto);
        }
    }

    /**
     * 刷新即将到期的缓存
     */
    private void refreshExpiringCache() {
        // 这里可以实现检查即将到期的缓存并刷新的逻辑
        // 具体实现取决于缓存的TTL策略
    }
}
```

#### 9.2.3 缓存穿透防护

在高并发场景下，缓存穿透可能导致大量请求直接访问数据库，造成系统性能问题。我们需要实现多层防护机制。

##### 9.2.3.1 布隆过滤器防护

**布隆过滤器缓存防护服务**

```java
/**
 * 布隆过滤器缓存防护服务
 * <p>
 * 使用布隆过滤器防止缓存穿透攻击。
 *
 * @author 开发者
 * @since 1.0.0
 */
@Service
@Slf4j
public class BloomFilterCacheGuard {

    private final BloomFilter<String> bloomFilter;
    private final RedisTemplate<String, Object> redisTemplate;
    private final UserRepository userRepository;
    private final ScheduledExecutorService scheduler;

    public BloomFilterCacheGuard(RedisTemplate<String, Object> redisTemplate,
                               UserRepository userRepository) {
        this.redisTemplate = redisTemplate;
        this.userRepository = userRepository;
        this.scheduler = Executors.newScheduledThreadPool(2);
        
        // 初始化布隆过滤器（预期100万个元素，误判率0.01%）
        this.bloomFilter = BloomFilter.create(
            Funnels.stringFunnel(StandardCharsets.UTF_8),
            1_000_000,
            0.0001
        );
        
        // 初始化布隆过滤器数据
        initializeBloomFilter();
        
        // 定时刷新布隆过滤器
        scheduleBloomFilterRefresh();
    }

    /**
     * 安全获取用户信息（带布隆过滤器防护）
     *
     * @param userId 用户ID
     * @return 用户信息
     */
    public UserDto getUserSafely(Long userId) {
        String userKey = "user:" + userId;
        
        // 第一层防护：布隆过滤器检查
        if (!bloomFilter.mightContain(userKey)) {
            log.debug("布隆过滤器拦截不存在的用户查询: {}", userId);
            return null;
        }
        
        // 第二层防护：缓存查询
        UserDto cachedUser = (UserDto) redisTemplate.opsForValue().get(userKey);
        if (cachedUser != null) {
            return cachedUser;
        }
        
        // 第三层防护：空值缓存检查
        String nullKey = "null:" + userKey;
        if (redisTemplate.hasKey(nullKey)) {
            log.debug("空值缓存命中，用户不存在: {}", userId);
            return null;
        }
        
        // 数据库查询
        User user = userRepository.selectById(userId);
        if (user != null) {
            UserDto userDto = convertToUserDto(user);
            // 缓存用户信息
            redisTemplate.opsForValue().set(userKey, userDto, Duration.ofHours(1));
            return userDto;
        } else {
            // 缓存空值，防止重复查询
            redisTemplate.opsForValue().set(nullKey, "null", Duration.ofMinutes(5));
            return null;
        }
    }

    /**
     * 初始化布隆过滤器
     */
    private void initializeBloomFilter() {
        log.info("开始初始化布隆过滤器");
        
        // 分批加载所有用户ID
        int batchSize = 1000;
        int offset = 0;
        int totalCount = 0;
        
        while (true) {
            LambdaQueryWrapper<User> queryWrapper = new LambdaQueryWrapper<>();
            queryWrapper.select(User::getId);
            queryWrapper.last("LIMIT " + offset + ", " + batchSize);
            
            List<User> users = userRepository.list(queryWrapper);
            if (users.isEmpty()) {
                break;
            }
            
            for (User user : users) {
                String userKey = "user:" + user.getId();
                bloomFilter.put(userKey);
                totalCount++;
            }
            
            offset += batchSize;
        }
        
        log.info("布隆过滤器初始化完成，加载用户数: {}", totalCount);
    }

    /**
     * 定时刷新布隆过滤器
     */
    private void scheduleBloomFilterRefresh() {
        // 每小时刷新一次布隆过滤器
        scheduler.scheduleAtFixedRate(() -> {
            try {
                refreshBloomFilter();
            } catch (Exception e) {
                log.error("刷新布隆过滤器失败", e);
            }
        }, 1, 1, TimeUnit.HOURS);
    }

    /**
     * 刷新布隆过滤器
     */
    private void refreshBloomFilter() {
        log.info("开始刷新布隆过滤器");
        
        // 查询新增的用户
        Instant oneHourAgo = Instant.now().minus(1, ChronoUnit.HOURS);
        LambdaQueryWrapper<User> queryWrapper = new LambdaQueryWrapper<>();
        queryWrapper.select(User::getId);
        queryWrapper.gt(User::getCreatedAt, oneHourAgo);
        
        List<User> newUsers = userRepository.list(queryWrapper);
        
        for (User user : newUsers) {
            String userKey = "user:" + user.getId();
            bloomFilter.put(userKey);
        }
        
        log.info("布隆过滤器刷新完成，新增用户数: {}", newUsers.size());
    }

    /**
     * 添加新用户到布隆过滤器
     *
     * @param userId 用户ID
     */
    public void addUserToBloomFilter(Long userId) {
        String userKey = "user:" + userId;
        bloomFilter.put(userKey);
        log.debug("添加用户到布隆过滤器: {}", userId);
    }
}
```

##### 9.2.3.2 分布式锁防护

**分布式锁缓存防护服务**

```java
/**
 * 分布式锁缓存防护服务
 * <p>
 * 使用分布式锁防止缓存击穿和雪崩。
 *
 * @author 开发者
 * @since 1.0.0
 */
@Service
@Slf4j
public class DistributedLockCacheGuard {

    private final RedisTemplate<String, Object> redisTemplate;
    private final RedissonClient redissonClient;
    private final UserRepository userRepository;

    public DistributedLockCacheGuard(RedisTemplate<String, Object> redisTemplate,
                                   RedissonClient redissonClient,
                                   UserRepository userRepository) {
        this.redisTemplate = redisTemplate;
        this.redissonClient = redissonClient;
        this.userRepository = userRepository;
    }

    /**
     * 安全获取用户信息（带分布式锁防护）
     *
     * @param userId 用户ID
     * @return 用户信息
     */
    public UserDto getUserWithLock(Long userId) {
        String cacheKey = "user:" + userId;
        
        // 尝试从缓存获取
        UserDto cachedUser = (UserDto) redisTemplate.opsForValue().get(cacheKey);
        if (cachedUser != null) {
            return cachedUser;
        }
        
        // 缓存未命中，使用分布式锁防止击穿
        String lockKey = "lock:user:" + userId;
        RLock lock = redissonClient.getLock(lockKey);
        
        try {
            // 尝试获取锁，最多等待3秒，锁定10秒
            if (lock.tryLock(3, 10, TimeUnit.SECONDS)) {
                try {
                    // 双重检查，防止重复查询
                    cachedUser = (UserDto) redisTemplate.opsForValue().get(cacheKey);
                    if (cachedUser != null) {
                        return cachedUser;
                    }
                    
                    // 从数据库查询
                    User user = userRepository.selectById(userId);
                    if (user != null) {
                        UserDto userDto = convertToUserDto(user);
                        
                        // 设置随机过期时间，防止缓存雪崩
                        Duration expiration = getRandomExpiration(Duration.ofHours(1));
                        redisTemplate.opsForValue().set(cacheKey, userDto, expiration);
                        
                        return userDto;
                    } else {
                        // 缓存空值
                        redisTemplate.opsForValue().set(cacheKey, new NullValue(), Duration.ofMinutes(5));
                        return null;
                    }
                } finally {
                    lock.unlock();
                }
            } else {
                // 获取锁失败，返回降级结果
                log.warn("获取分布式锁失败，返回降级结果: userId={}", userId);
                return getFallbackUser(userId);
            }
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
            log.error("获取分布式锁被中断: userId={}", userId, e);
            return getFallbackUser(userId);
        }
    }

    /**
     * 获取随机过期时间（防止缓存雪崩）
     *
     * @param baseDuration 基础过期时间
     * @return 随机过期时间
     */
    private Duration getRandomExpiration(Duration baseDuration) {
        // 在基础时间上增加0-30%的随机时间
        long baseSeconds = baseDuration.getSeconds();
        long randomSeconds = (long) (baseSeconds * 0.3 * Math.random());
        return Duration.ofSeconds(baseSeconds + randomSeconds);
    }

    /**
     * 获取降级用户信息
     *
     * @param userId 用户ID
     * @return 降级用户信息
     */
    private UserDto getFallbackUser(Long userId) {
        // 返回基础的用户信息或默认值
        return UserDto.builder()
                .id(userId)
                .username("用户" + userId)
                .status("UNKNOWN")
                .build();
    }

    /**
     * 空值标记类
     */
    private static class NullValue {
        // 用于标记缓存中的空值
    }
}
```

### 9.3 并发优化

#### 9.3.1 线程池配置

在DDD分层架构中，合理配置线程池可以显著提升系统的并发处理能力。

##### 9.3.1.1 自定义线程池配置

**线程池配置管理**

```java
/**
 * 线程池配置管理
 * <p>
 * 为不同业务场景配置专用线程池。
 *
 * @author 开发者
 * @since 1.0.0
 */
@Configuration
@EnableAsync
@Slf4j
public class ThreadPoolConfiguration {

    @Value("${app.thread-pool.core-pool-size:10}")
    private int corePoolSize;

    @Value("${app.thread-pool.max-pool-size:50}")
    private int maxPoolSize;

    @Value("${app.thread-pool.queue-capacity:1000}")
    private int queueCapacity;

    @Value("${app.thread-pool.keep-alive-seconds:60}")
    private int keepAliveSeconds;

    /**
     * 默认异步任务线程池
     */
    @Bean("taskExecutor")
    @Primary
    public ThreadPoolTaskExecutor taskExecutor() {
        ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor();
        executor.setCorePoolSize(corePoolSize);
        executor.setMaxPoolSize(maxPoolSize);
        executor.setQueueCapacity(queueCapacity);
        executor.setKeepAliveSeconds(keepAliveSeconds);
        executor.setThreadNamePrefix("async-task-");
        executor.setRejectedExecutionHandler(new ThreadPoolExecutor.CallerRunsPolicy());
        executor.setWaitForTasksToCompleteOnShutdown(true);
        executor.setAwaitTerminationSeconds(30);
        executor.initialize();
        
        log.info("默认线程池配置完成: core={}, max={}, queue={}", 
                corePoolSize, maxPoolSize, queueCapacity);
        
        return executor;
    }

    /**
     * IO密集型任务线程池
     */
    @Bean("ioTaskExecutor")
    public ThreadPoolTaskExecutor ioTaskExecutor() {
        ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor();
        // IO密集型任务，线程数可以设置更大
        executor.setCorePoolSize(corePoolSize * 2);
        executor.setMaxPoolSize(maxPoolSize * 2);
        executor.setQueueCapacity(queueCapacity);
        executor.setKeepAliveSeconds(keepAliveSeconds);
        executor.setThreadNamePrefix("io-task-");
        executor.setRejectedExecutionHandler(new ThreadPoolExecutor.CallerRunsPolicy());
        executor.setWaitForTasksToCompleteOnShutdown(true);
        executor.setAwaitTerminationSeconds(30);
        executor.initialize();
        
        return executor;
    }

    /**
     * CPU密集型任务线程池
     */
    @Bean("cpuTaskExecutor")
    public ThreadPoolTaskExecutor cpuTaskExecutor() {
        ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor();
        // CPU密集型任务，线程数通常设置为CPU核心数
        int cpuCores = Runtime.getRuntime().availableProcessors();
        executor.setCorePoolSize(cpuCores);
        executor.setMaxPoolSize(cpuCores);
        executor.setQueueCapacity(queueCapacity);
        executor.setKeepAliveSeconds(keepAliveSeconds);
        executor.setThreadNamePrefix("cpu-task-");
        executor.setRejectedExecutionHandler(new ThreadPoolExecutor.AbortPolicy());
        executor.setWaitForTasksToCompleteOnShutdown(true);
        executor.setAwaitTerminationSeconds(30);
        executor.initialize();
        
        return executor;
    }

    /**
     * 事件处理线程池
     */
    @Bean("eventTaskExecutor")
    public ThreadPoolTaskExecutor eventTaskExecutor() {
        ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor();
        executor.setCorePoolSize(5);
        executor.setMaxPoolSize(20);
        executor.setQueueCapacity(500);
        executor.setKeepAliveSeconds(60);
        executor.setThreadNamePrefix("event-task-");
        executor.setRejectedExecutionHandler(new ThreadPoolExecutor.CallerRunsPolicy());
        executor.setWaitForTasksToCompleteOnShutdown(true);
        executor.setAwaitTerminationSeconds(30);
        executor.initialize();
        
        return executor;
    }
}
```

#### 9.3.2 异步处理

##### 9.3.2.1 异步任务处理服务

**并发任务处理服务**

```java
/**
 * 并发任务处理服务
 * <p>
 * 提供各种并发处理模式的实现。
 *
 * @author 开发者
 * @since 1.0.0
 */
@Service
@Slf4j
public class ConcurrentTaskService {

    private final ThreadPoolTaskExecutor taskExecutor;
    private final ThreadPoolTaskExecutor ioTaskExecutor;
    private final ThreadPoolTaskExecutor cpuTaskExecutor;
    private final UserRepository userRepository;
    private final OrderRepository orderRepository;

    public ConcurrentTaskService(@Qualifier("taskExecutor") ThreadPoolTaskExecutor taskExecutor,
                               @Qualifier("ioTaskExecutor") ThreadPoolTaskExecutor ioTaskExecutor,
                               @Qualifier("cpuTaskExecutor") ThreadPoolTaskExecutor cpuTaskExecutor,
                               UserRepository userRepository,
                               OrderRepository orderRepository) {
        this.taskExecutor = taskExecutor;
        this.ioTaskExecutor = ioTaskExecutor;
        this.cpuTaskExecutor = cpuTaskExecutor;
        this.userRepository = userRepository;
        this.orderRepository = orderRepository;
    }

    /**
     * 并行处理用户数据
     *
     * @param userIds 用户ID列表
     * @return 处理结果
     */
    public CompletableFuture<List<UserProcessResult>> processUsersParallel(List<Long> userIds) {
        if (userIds.isEmpty()) {
            return CompletableFuture.completedFuture(Collections.emptyList());
        }
        
        // 将用户ID分批处理
        int batchSize = 100;
        List<List<Long>> batches = partitionList(userIds, batchSize);
        
        // 创建并行任务
        List<CompletableFuture<List<UserProcessResult>>> futures = batches.stream()
                .map(batch -> CompletableFuture.supplyAsync(() -> processBatchUsers(batch), ioTaskExecutor))
                .collect(Collectors.toList());
        
        // 等待所有任务完成并合并结果
        return CompletableFuture.allOf(futures.toArray(new CompletableFuture[0]))
                .thenApply(v -> futures.stream()
                        .map(CompletableFuture::join)
                        .flatMap(List::stream)
                        .collect(Collectors.toList()));
    }

    /**
     * 并行聚合数据
     *
     * @param userId 用户ID
     * @return 聚合结果
     */
    public CompletableFuture<UserAggregateData> aggregateUserDataParallel(Long userId) {
        // 并行获取用户的各种数据
        CompletableFuture<User> userFuture = CompletableFuture
                .supplyAsync(() -> userRepository.selectById(userId), ioTaskExecutor);
        
        CompletableFuture<List<Order>> ordersFuture = CompletableFuture
                .supplyAsync(() -> getUserOrders(userId), ioTaskExecutor);
        
        CompletableFuture<UserStatistics> statsFuture = CompletableFuture
                .supplyAsync(() -> getUserStatistics(userId), ioTaskExecutor);
        
        CompletableFuture<List<String>> permissionsFuture = CompletableFuture
                .supplyAsync(() -> getUserPermissions(userId), ioTaskExecutor);
        
        // 等待所有数据获取完成，然后聚合
        return CompletableFuture.allOf(userFuture, ordersFuture, statsFuture, permissionsFuture)
                .thenApplyAsync(v -> {
                    User user = userFuture.join();
                    List<Order> orders = ordersFuture.join();
                    UserStatistics stats = statsFuture.join();
                    List<String> permissions = permissionsFuture.join();
                    
                    return UserAggregateData.builder()
                            .user(user)
                            .orders(orders)
                            .statistics(stats)
                            .permissions(permissions)
                            .build();
                }, cpuTaskExecutor);
    }

    /**
     * 分割列表
     *
     * @param list 原始列表
     * @param batchSize 批次大小
     * @return 分割后的列表
     */
    private <T> List<List<T>> partitionList(List<T> list, int batchSize) {
        List<List<T>> partitions = new ArrayList<>();
        for (int i = 0; i < list.size(); i += batchSize) {
            partitions.add(list.subList(i, Math.min(i + batchSize, list.size())));
        }
        return partitions;
    }
}
```

#### 9.3.3 锁优化

##### 9.3.3.1 分段锁实现

**分段锁管理器**

```java
/**
 * 分段锁管理器
 * <p>
 * 使用分段锁减少锁竞争，提高并发性能。
 *
 * @author 开发者
 * @since 1.0.0
 */
@Component
@Slf4j
public class SegmentedLockManager {

    private final int segmentCount;
    private final ReentrantReadWriteLock[] locks;
    private final ConcurrentHashMap<String, Object> lockObjects;

    public SegmentedLockManager(@Value("${app.lock.segment-count:16}") int segmentCount) {
        this.segmentCount = segmentCount;
        this.locks = new ReentrantReadWriteLock[segmentCount];
        this.lockObjects = new ConcurrentHashMap<>();
        
        // 初始化分段锁
        for (int i = 0; i < segmentCount; i++) {
            locks[i] = new ReentrantReadWriteLock();
        }
        
        log.info("分段锁管理器初始化完成，分段数: {}", segmentCount);
    }

    /**
     * 获取分段索引
     *
     * @param key 键
     * @return 分段索引
     */
    private int getSegmentIndex(String key) {
        return Math.abs(key.hashCode()) % segmentCount;
    }

    /**
     * 执行读操作
     *
     * @param key 键
     * @param operation 读操作
     * @return 操作结果
     */
    public <T> T executeRead(String key, Supplier<T> operation) {
        int segmentIndex = getSegmentIndex(key);
        ReentrantReadWriteLock.ReadLock readLock = locks[segmentIndex].readLock();
        
        readLock.lock();
        try {
            return operation.get();
        } finally {
            readLock.unlock();
        }
    }

    /**
     * 执行写操作
     *
     * @param key 键
     * @param operation 写操作
     * @return 操作结果
     */
    public <T> T executeWrite(String key, Supplier<T> operation) {
        int segmentIndex = getSegmentIndex(key);
        ReentrantReadWriteLock.WriteLock writeLock = locks[segmentIndex].writeLock();
        
        writeLock.lock();
        try {
            return operation.get();
        } finally {
            writeLock.unlock();
        }
    }

    /**
     * 获取锁统计信息
     *
     * @return 锁统计信息
     */
    public LockStatistics getStatistics() {
        int totalQueuedThreads = 0;
        int totalWaitingReaders = 0;
        int totalWaitingWriters = 0;
        
        for (ReentrantReadWriteLock lock : locks) {
            totalQueuedThreads += lock.getQueueLength();
            totalWaitingReaders += lock.getReadLockCount();
            if (lock.isWriteLocked()) {
                totalWaitingWriters++;
            }
        }
        
        return LockStatistics.builder()
                .segmentCount(segmentCount)
                .totalQueuedThreads(totalQueuedThreads)
                .totalWaitingReaders(totalWaitingReaders)
                .totalWaitingWriters(totalWaitingWriters)
                .objectLockCount(lockObjects.size())
                .build();
    }
}
```

### 9.4 资源优化

#### 9.4.1 连接池优化

##### 9.4.1.1 数据库连接池配置

**HikariCP连接池优化配置**

```yaml
# application.yml
spring:
  datasource:
    type: com.zaxxer.hikari.HikariDataSource
    hikari:
      # 连接池配置
      minimum-idle: 10                    # 最小空闲连接数
      maximum-pool-size: 50               # 最大连接池大小
      connection-timeout: 30000           # 连接超时时间(毫秒)
      idle-timeout: 600000                # 空闲连接超时时间(毫秒)
      max-lifetime: 1800000               # 连接最大生命周期(毫秒)
      leak-detection-threshold: 60000     # 连接泄漏检测阈值(毫秒)
      
      # 性能优化配置
      auto-commit: false                  # 关闭自动提交
      connection-test-query: SELECT 1     # 连接测试查询
      validation-timeout: 5000            # 连接验证超时时间
      
      # 连接池监控
      register-mbeans: true               # 启用JMX监控
      pool-name: HikariPool-Main          # 连接池名称

# Redis连接池配置
spring:
  redis:
    lettuce:
      pool:
        max-active: 50                    # 最大连接数
        max-idle: 20                      # 最大空闲连接数
        min-idle: 5                       # 最小空闲连接数
        max-wait: 3000ms                  # 最大等待时间
      shutdown-timeout: 100ms             # 关闭超时时间
    timeout: 3000ms                       # 连接超时时间
```

##### 9.4.1.2 连接池监控服务

**连接池监控服务**

```java
/**
 * 连接池监控服务
 * <p>
 * 监控数据库和Redis连接池的使用情况。
 *
 * @author 开发者
 * @since 1.0.0
 */
@Service
@Slf4j
public class ConnectionPoolMonitoringService {

    private final HikariDataSource dataSource;
    private final LettuceConnectionFactory redisConnectionFactory;
    private final MeterRegistry meterRegistry;

    public ConnectionPoolMonitoringService(HikariDataSource dataSource,
                                         LettuceConnectionFactory redisConnectionFactory,
                                         MeterRegistry meterRegistry) {
        this.dataSource = dataSource;
        this.redisConnectionFactory = redisConnectionFactory;
        this.meterRegistry = meterRegistry;
        
        // 注册连接池指标
        registerConnectionPoolMetrics();
    }

    /**
     * 注册连接池指标
     */
    private void registerConnectionPoolMetrics() {
        // 数据库连接池指标
        Gauge.builder("hikari.connections.active")
                .description("Active database connections")
                .register(meterRegistry, dataSource, ds -> ds.getHikariPoolMXBean().getActiveConnections());
        
        Gauge.builder("hikari.connections.idle")
                .description("Idle database connections")
                .register(meterRegistry, dataSource, ds -> ds.getHikariPoolMXBean().getIdleConnections());
        
        Gauge.builder("hikari.connections.total")
                .description("Total database connections")
                .register(meterRegistry, dataSource, ds -> ds.getHikariPoolMXBean().getTotalConnections());
        
        Gauge.builder("hikari.connections.pending")
                .description("Pending database connection requests")
                .register(meterRegistry, dataSource, ds -> ds.getHikariPoolMXBean().getThreadsAwaitingConnection());
    }

    /**
     * 获取连接池状态
     *
     * @return 连接池状态
     */
    public ConnectionPoolStatus getConnectionPoolStatus() {
        HikariPoolMXBean poolMXBean = dataSource.getHikariPoolMXBean();
        
        return ConnectionPoolStatus.builder()
                .activeConnections(poolMXBean.getActiveConnections())
                .idleConnections(poolMXBean.getIdleConnections())
                .totalConnections(poolMXBean.getTotalConnections())
                .threadsAwaitingConnection(poolMXBean.getThreadsAwaitingConnection())
                .maximumPoolSize(dataSource.getMaximumPoolSize())
                .minimumIdle(dataSource.getMinimumIdle())
                .connectionTimeout(dataSource.getConnectionTimeout())
                .idleTimeout(dataSource.getIdleTimeout())
                .maxLifetime(dataSource.getMaxLifetime())
                .build();
    }

    /**
     * 检查连接池健康状态
     *
     * @return 健康状态
     */
    public HealthStatus checkConnectionPoolHealth() {
        try {
            ConnectionPoolStatus status = getConnectionPoolStatus();
            
            // 检查连接池使用率
            double usageRate = (double) status.getActiveConnections() / status.getMaximumPoolSize();
            
            if (usageRate > 0.9) {
                return HealthStatus.builder()
                        .status("CRITICAL")
                        .message("连接池使用率过高: " + String.format("%.2f%%", usageRate * 100))
                        .build();
            } else if (usageRate > 0.7) {
                return HealthStatus.builder()
                        .status("WARNING")
                        .message("连接池使用率较高: " + String.format("%.2f%%", usageRate * 100))
                        .build();
            } else {
                return HealthStatus.builder()
                        .status("HEALTHY")
                        .message("连接池状态正常")
                        .build();
            }
        } catch (Exception e) {
            log.error("检查连接池健康状态失败", e);
            return HealthStatus.builder()
                    .status("ERROR")
                    .message("无法获取连接池状态: " + e.getMessage())
                    .build();
        }
    }
}
```

#### 9.4.2 内存优化

##### 9.4.2.1 对象池管理

**对象池管理器**

```java
/**
 * 对象池管理器
 * <p>
 * 管理重量级对象的创建和复用，减少GC压力。
 *
 * @author 开发者
 * @since 1.0.0
 */
@Component
@Slf4j
public class ObjectPoolManager {

    private final Map<Class<?>, ObjectPool<?>> pools = new ConcurrentHashMap<>();
    private final ScheduledExecutorService cleanupExecutor;

    public ObjectPoolManager() {
        this.cleanupExecutor = Executors.newScheduledThreadPool(1);
        
        // 定时清理过期对象
        cleanupExecutor.scheduleAtFixedRate(this::cleanupExpiredObjects, 
                5, 5, TimeUnit.MINUTES);
    }

    /**
     * 获取对象池
     *
     * @param clazz 对象类型
     * @param factory 对象工厂
     * @param config 池配置
     * @return 对象池
     */
    @SuppressWarnings("unchecked")
    public <T> ObjectPool<T> getPool(Class<T> clazz, 
                                   Supplier<T> factory, 
                                   PoolConfig config) {
        return (ObjectPool<T>) pools.computeIfAbsent(clazz, 
                k -> new ObjectPool<>(factory, config));
    }

    /**
     * 从池中借用对象
     *
     * @param clazz 对象类型
     * @return 对象实例
     */
    @SuppressWarnings("unchecked")
    public <T> T borrowObject(Class<T> clazz) {
        ObjectPool<T> pool = (ObjectPool<T>) pools.get(clazz);
        if (pool == null) {
            throw new IllegalStateException("对象池未初始化: " + clazz.getName());
        }
        return pool.borrowObject();
    }

    /**
     * 归还对象到池中
     *
     * @param object 对象实例
     */
    @SuppressWarnings("unchecked")
    public <T> void returnObject(T object) {
        Class<T> clazz = (Class<T>) object.getClass();
        ObjectPool<T> pool = (ObjectPool<T>) pools.get(clazz);
        if (pool != null) {
            pool.returnObject(object);
        }
    }

    /**
     * 清理过期对象
     */
    private void cleanupExpiredObjects() {
        for (ObjectPool<?> pool : pools.values()) {
            pool.cleanup();
        }
    }
}
```

#### 9.4.3 GC 调优

##### 9.4.3.1 GC监控和调优

**GC监控服务**

```java
/**
 * GC监控服务
 * <p>
 * 监控JVM垃圾回收性能指标。
 *
 * @author 开发者
 * @since 1.0.0
 */
@Service
@Slf4j
public class GCMonitoringService {

    private final MeterRegistry meterRegistry;
    private final List<GarbageCollectorMXBean> gcBeans;
    private final MemoryMXBean memoryBean;

    public GCMonitoringService(MeterRegistry meterRegistry) {
        this.meterRegistry = meterRegistry;
        this.gcBeans = ManagementFactory.getGarbageCollectorMXBeans();
        this.memoryBean = ManagementFactory.getMemoryMXBean();
        
        // 注册GC指标
        registerGCMetrics();
    }

    /**
     * 注册GC指标
     */
    private void registerGCMetrics() {
        // 注册GC次数和时间指标
        for (GarbageCollectorMXBean gcBean : gcBeans) {
            Gauge.builder("jvm.gc.collections")
                    .tag("gc", gcBean.getName())
                    .description("GC collection count")
                    .register(meterRegistry, gcBean, GarbageCollectorMXBean::getCollectionCount);
            
            Gauge.builder("jvm.gc.time")
                    .tag("gc", gcBean.getName())
                    .description("GC collection time")
                    .register(meterRegistry, gcBean, GarbageCollectorMXBean::getCollectionTime);
        }
        
        // 注册内存使用指标
        Gauge.builder("jvm.memory.heap.used")
                .description("Heap memory used")
                .register(meterRegistry, memoryBean, bean -> bean.getHeapMemoryUsage().getUsed());
        
        Gauge.builder("jvm.memory.heap.max")
                .description("Heap memory max")
                .register(meterRegistry, memoryBean, bean -> bean.getHeapMemoryUsage().getMax());
        
        Gauge.builder("jvm.memory.nonheap.used")
                .description("Non-heap memory used")
                .register(meterRegistry, memoryBean, bean -> bean.getNonHeapMemoryUsage().getUsed());
    }

    /**
     * 获取GC统计信息
     *
     * @return GC统计信息
     */
    public GCStatistics getGCStatistics() {
        List<GCInfo> gcInfos = new ArrayList<>();
        
        for (GarbageCollectorMXBean gcBean : gcBeans) {
            gcInfos.add(GCInfo.builder()
                    .name(gcBean.getName())
                    .collectionCount(gcBean.getCollectionCount())
                    .collectionTime(gcBean.getCollectionTime())
                    .build());
        }
        
        MemoryUsage heapUsage = memoryBean.getHeapMemoryUsage();
        MemoryUsage nonHeapUsage = memoryBean.getNonHeapMemoryUsage();
        
        return GCStatistics.builder()
                .gcInfos(gcInfos)
                .heapUsed(heapUsage.getUsed())
                .heapMax(heapUsage.getMax())
                .heapCommitted(heapUsage.getCommitted())
                .nonHeapUsed(nonHeapUsage.getUsed())
                .nonHeapMax(nonHeapUsage.getMax())
                .nonHeapCommitted(nonHeapUsage.getCommitted())
                .timestamp(Instant.now())
                .build();
    }

    /**
     * 分析GC性能
     *
     * @return GC性能分析结果
     */
    public GCPerformanceAnalysis analyzeGCPerformance() {
        GCStatistics stats = getGCStatistics();
        
        // 计算GC频率和平均时间
        long totalCollections = stats.getGcInfos().stream()
                .mapToLong(GCInfo::getCollectionCount)
                .sum();
        
        long totalTime = stats.getGcInfos().stream()
                .mapToLong(GCInfo::getCollectionTime)
                .sum();
        
        double avgGCTime = totalCollections > 0 ? (double) totalTime / totalCollections : 0;
        
        // 计算内存使用率
        double heapUsageRate = (double) stats.getHeapUsed() / stats.getHeapMax();
        
        // 生成建议
        List<String> recommendations = new ArrayList<>();
        
        if (heapUsageRate > 0.8) {
            recommendations.add("堆内存使用率过高，建议增加堆内存大小");
        }
        
        if (avgGCTime > 100) {
            recommendations.add("GC平均时间过长，建议优化GC参数或减少对象创建");
        }
        
        if (totalCollections > 1000) {
            recommendations.add("GC频率过高，建议检查内存泄漏或优化对象生命周期");
        }
        
        return GCPerformanceAnalysis.builder()
                .totalCollections(totalCollections)
                .totalTime(totalTime)
                .avgGCTime(avgGCTime)
                .heapUsageRate(heapUsageRate)
                .recommendations(recommendations)
                .analysisTime(Instant.now())
                .build();
    }
}
```

**JVM调优配置示例**

```bash
# 生产环境JVM参数配置
JAVA_OPTS="
# 堆内存配置
-Xms2g -Xmx4g
-XX:NewRatio=3
-XX:SurvivorRatio=8

# GC配置 (G1GC)
-XX:+UseG1GC
-XX:MaxGCPauseMillis=200
-XX:G1HeapRegionSize=16m
-XX:G1NewSizePercent=30
-XX:G1MaxNewSizePercent=40

# GC日志配置
-XX:+PrintGC
-XX:+PrintGCDetails
-XX:+PrintGCTimeStamps
-XX:+PrintGCApplicationStoppedTime
-Xloggc:/var/log/app/gc.log
-XX:+UseGCLogFileRotation
-XX:NumberOfGCLogFiles=5
-XX:GCLogFileSize=100M

# 内存溢出处理
-XX:+HeapDumpOnOutOfMemoryError
-XX:HeapDumpPath=/var/log/app/heapdump.hprof

# 性能监控
-XX:+UnlockCommercialFeatures
-XX:+FlightRecorder
-XX:StartFlightRecording=duration=60s,filename=/var/log/app/flight.jfr

# 其他优化参数
-XX:+UseStringDeduplication
-XX:+OptimizeStringConcat
-server
"
```

通过以上的性能优化实现，我们为DDD分层架构提供了全面的性能优化方案，包括：

1. **缓存穿透防护**：布隆过滤器、分布式锁防护
2. **并发优化**：线程池配置、异步处理、分段锁
3. **资源优化**：连接池优化、对象池管理、GC调优

这些优化策略可以显著提升系统的性能和并发处理能力，为高负载场景提供强有力的支撑。