## 11. 部署运维

### 11. 部署运维

### 11.1 容器化部署

#### 11.1.1 Docker

**Dockerfile 配置**

```dockerfile
# 多阶段构建 Dockerfile
FROM maven:3.8.6-openjdk-17-slim AS builder

WORKDIR /app
COPY pom.xml .
COPY src ./src

# 构建应用
RUN mvn clean package -DskipTests

# 运行时镜像
FROM openjdk:17-jre-slim

# 创建应用用户
RUN groupadd -r appuser && useradd -r -g appuser appuser

# 安装必要工具
RUN apt-get update && apt-get install -y \
    curl \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

# 复制应用文件
COPY --from=builder /app/target/*.jar app.jar

# 创建日志目录
RUN mkdir -p /app/logs && chown -R appuser:appuser /app

# 切换到非root用户
USER appuser

# 健康检查
HEALTHCHECK --interval=30s --timeout=3s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:8080/actuator/health || exit 1

# 暴露端口
EXPOSE 8080

# JVM 参数优化
ENV JAVA_OPTS="-Xms512m -Xmx1024m -XX:+UseG1GC -XX:+UseContainerSupport"

# 启动应用
ENTRYPOINT ["sh", "-c", "java $JAVA_OPTS -jar app.jar"]
```

**Docker Compose 配置**

```yaml
# docker-compose.yml
version: '3.8'

services:
  app:
    build: .
    ports:
      - "8080:8080"
    environment:
      - SPRING_PROFILES_ACTIVE=docker
      - SPRING_DATASOURCE_URL=jdbc:mysql://mysql:3306/ddd_demo
      - SPRING_REDIS_HOST=redis
    depends_on:
      mysql:
        condition: service_healthy
      redis:
        condition: service_healthy
    volumes:
      - ./logs:/app/logs
    networks:
      - app-network
    restart: unless-stopped

  mysql:
    image: mysql:8.0
    environment:
      MYSQL_ROOT_PASSWORD: root123
      MYSQL_DATABASE: ddd_demo
      MYSQL_USER: app_user
      MYSQL_PASSWORD: app_pass
    ports:
      - "3306:3306"
    volumes:
      - mysql_data:/var/lib/mysql
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost"]
      timeout: 20s
      retries: 10
    networks:
      - app-network

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 3s
      retries: 3
    networks:
      - app-network

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./ssl:/etc/nginx/ssl
    depends_on:
      - app
    networks:
      - app-network

volumes:
  mysql_data:
  redis_data:

networks:
  app-network:
    driver: bridge
```

#### 11.1.2 Kubernetes

**Deployment 配置**

```yaml
# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ddd-demo-app
  namespace: production
  labels:
    app: ddd-demo
    version: v1.0.0
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: ddd-demo
  template:
    metadata:
      labels:
        app: ddd-demo
        version: v1.0.0
    spec:
      containers:
      - name: app
        image: ddd-demo:v1.0.0
        ports:
        - containerPort: 8080
          name: http
        env:
        - name: SPRING_PROFILES_ACTIVE
          value: "k8s"
        - name: SPRING_DATASOURCE_URL
          valueFrom:
            secretKeyRef:
              name: db-secret
              key: url
        - name: SPRING_DATASOURCE_USERNAME
          valueFrom:
            secretKeyRef:
              name: db-secret
              key: username
        - name: SPRING_DATASOURCE_PASSWORD
          valueFrom:
            secretKeyRef:
              name: db-secret
              key: password
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /actuator/health/liveness
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 5
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /actuator/health/readiness
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 3
          failureThreshold: 3
        volumeMounts:
        - name: config-volume
          mountPath: /app/config
        - name: logs-volume
          mountPath: /app/logs
      volumes:
      - name: config-volume
        configMap:
          name: app-config
      - name: logs-volume
        emptyDir: {}
      imagePullSecrets:
      - name: registry-secret
```

**Service 配置**

```yaml
# k8s/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: ddd-demo-service
  namespace: production
  labels:
    app: ddd-demo
spec:
  type: ClusterIP
  ports:
  - port: 80
    targetPort: 8080
    protocol: TCP
    name: http
  selector:
    app: ddd-demo
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ddd-demo-ingress
  namespace: production
  annotations:
    kubernetes.io/ingress.class: nginx
    cert-manager.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/rate-limit: "100"
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
spec:
  tls:
  - hosts:
    - api.example.com
    secretName: ddd-demo-tls
  rules:
  - host: api.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: ddd-demo-service
            port:
              number: 80
```

### 11.2 环境管理

#### 11.2.1 配置管理

**ConfigMap 配置**

```yaml
# k8s/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
  namespace: production
data:
  application.yml: |
    server:
      port: 8080
      servlet:
        context-path: /api
    
    spring:
      application:
        name: ddd-demo
      
      datasource:
        hikari:
          maximum-pool-size: 20
          minimum-idle: 5
          connection-timeout: 30000
          idle-timeout: 600000
          max-lifetime: 1800000
      
      redis:
        timeout: 3000
        lettuce:
          pool:
            max-active: 20
            max-idle: 8
            min-idle: 2
    
    logging:
      level:
        com.example: INFO
        org.springframework.security: DEBUG
      pattern:
        console: "%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n"
        file: "%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n"
      file:
        name: /app/logs/application.log
        max-size: 100MB
        max-history: 30
    
    management:
      endpoints:
        web:
          exposure:
            include: health,info,metrics,prometheus
      endpoint:
        health:
          show-details: always
      metrics:
        export:
          prometheus:
            enabled: true
```

**Secret 配置**

```yaml
# k8s/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: db-secret
  namespace: production
type: Opaque
data:
  url: amRiYzpteXNxbDovL215c3FsOjMzMDYvZGRkX2RlbW8=  # base64 encoded
  username: YXBwX3VzZXI=  # base64 encoded
  password: YXBwX3Bhc3M=  # base64 encoded
---
apiVersion: v1
kind: Secret
metadata:
  name: jwt-secret
  namespace: production
type: Opaque
data:
  secret-key: bXlfc2VjcmV0X2tleV9mb3Jfand0X3Rva2Vu  # base64 encoded
```

#### 11.2.2 环境隔离

**命名空间配置**

```yaml
# k8s/namespaces.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: development
  labels:
    environment: dev
---
apiVersion: v1
kind: Namespace
metadata:
  name: staging
  labels:
    environment: staging
---
apiVersion: v1
kind: Namespace
metadata:
  name: production
  labels:
    environment: prod
```

**资源配额配置**

```yaml
# k8s/resource-quota.yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: production
spec:
  hard:
    requests.cpu: "4"
    requests.memory: 8Gi
    limits.cpu: "8"
    limits.memory: 16Gi
    persistentvolumeclaims: "10"
    services: "5"
    secrets: "10"
    configmaps: "10"
---
apiVersion: v1
kind: LimitRange
metadata:
  name: limit-range
  namespace: production
spec:
  limits:
  - default:
      cpu: "500m"
      memory: "1Gi"
    defaultRequest:
      cpu: "250m"
      memory: "512Mi"
    type: Container
```

#### 11.2.3 蓝绿部署

**蓝绿部署脚本**

```bash
#!/bin/bash
# deploy.sh

set -e

NAMESPACE=${1:-production}
VERSION=${2:-latest}
DEPLOYMENT_NAME="ddd-demo-app"
SERVICE_NAME="ddd-demo-service"

echo "开始蓝绿部署到命名空间: $NAMESPACE"
echo "部署版本: $VERSION"

# 获取当前活跃版本
CURRENT_VERSION=$(kubectl get deployment $DEPLOYMENT_NAME -n $NAMESPACE -o jsonpath='{.metadata.labels.version}' 2>/dev/null || echo "none")
echo "当前版本: $CURRENT_VERSION"

# 确定新版本颜色
if [[ "$CURRENT_VERSION" == *"blue"* ]]; then
    NEW_COLOR="green"
    OLD_COLOR="blue"
else
    NEW_COLOR="blue"
    OLD_COLOR="green"
fi

NEW_DEPLOYMENT_NAME="${DEPLOYMENT_NAME}-${NEW_COLOR}"
OLD_DEPLOYMENT_NAME="${DEPLOYMENT_NAME}-${OLD_COLOR}"

echo "部署新版本到: $NEW_COLOR"

# 创建新版本部署
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: $NEW_DEPLOYMENT_NAME
  namespace: $NAMESPACE
  labels:
    app: ddd-demo
    version: $VERSION
    color: $NEW_COLOR
spec:
  replicas: 3
  selector:
    matchLabels:
      app: ddd-demo
      color: $NEW_COLOR
  template:
    metadata:
      labels:
        app: ddd-demo
        version: $VERSION
        color: $NEW_COLOR
    spec:
      containers:
      - name: app
        image: ddd-demo:$VERSION
        ports:
        - containerPort: 8080
        # ... 其他配置
EOF

# 等待新版本就绪
echo "等待新版本就绪..."
kubectl rollout status deployment/$NEW_DEPLOYMENT_NAME -n $NAMESPACE --timeout=300s

# 健康检查
echo "执行健康检查..."
for i in {1..30}; do
    if kubectl get pods -n $NAMESPACE -l color=$NEW_COLOR -o jsonpath='{.items[*].status.phase}' | grep -q "Running"; then
        echo "新版本健康检查通过"
        break
    fi
    echo "等待健康检查... ($i/30)"
    sleep 10
done

# 切换流量
echo "切换流量到新版本..."
kubectl patch service $SERVICE_NAME -n $NAMESPACE -p '{"spec":{"selector":{"color":"'$NEW_COLOR'"}}}'

# 验证切换
echo "验证流量切换..."
sleep 30

# 清理旧版本
read -p "是否删除旧版本 ($OLD_COLOR)? (y/N): " -n 1 -r
echo
if [[ $REPLY =~ ^[Yy]$ ]]; then
    kubectl delete deployment $OLD_DEPLOYMENT_NAME -n $NAMESPACE --ignore-not-found=true
    echo "旧版本已删除"
else
    echo "保留旧版本，可手动删除: kubectl delete deployment $OLD_DEPLOYMENT_NAME -n $NAMESPACE"
fi

echo "蓝绿部署完成!"
```

### 11.3 监控告警

#### 11.3.1 应用监控

**Prometheus 配置**

```yaml
# monitoring/prometheus-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitoring
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
    
    rule_files:
      - "alert_rules.yml"
    
    alerting:
      alertmanagers:
        - static_configs:
            - targets:
              - alertmanager:9093
    
    scrape_configs:
      - job_name: 'ddd-demo'
        kubernetes_sd_configs:
          - role: endpoints
            namespaces:
              names:
                - production
        relabel_configs:
          - source_labels: [__meta_kubernetes_service_name]
            action: keep
            regex: ddd-demo-service
          - source_labels: [__meta_kubernetes_endpoint_port_name]
            action: keep
            regex: http
        metrics_path: /actuator/prometheus
        scrape_interval: 30s
      
      - job_name: 'kubernetes-nodes'
        kubernetes_sd_configs:
          - role: node
        relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)
```

**告警规则配置**

```yaml
# monitoring/alert-rules.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: alert-rules
  namespace: monitoring
data:
  alert_rules.yml: |
    groups:
    - name: ddd-demo-alerts
      rules:
      - alert: HighErrorRate
        expr: rate(http_server_requests_seconds_count{status=~"5.."}[5m]) > 0.1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "高错误率告警"
          description: "应用 {{ $labels.instance }} 错误率超过 10%"
      
      - alert: HighResponseTime
        expr: histogram_quantile(0.95, rate(http_server_requests_seconds_bucket[5m])) > 2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "响应时间过长"
          description: "应用 {{ $labels.instance }} 95% 响应时间超过 2 秒"
      
      - alert: HighMemoryUsage
        expr: (jvm_memory_used_bytes / jvm_memory_max_bytes) > 0.8
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "内存使用率过高"
          description: "应用 {{ $labels.instance }} 内存使用率超过 80%"
      
      - alert: DatabaseConnectionPoolExhausted
        expr: hikaricp_connections_active >= hikaricp_connections_max
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "数据库连接池耗尽"
          description: "应用 {{ $labels.instance }} 数据库连接池已满"
```

#### 11.3.2 基础设施监控

**Node Exporter 配置**

```yaml
# monitoring/node-exporter.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-exporter
  namespace: monitoring
spec:
  selector:
    matchLabels:
      app: node-exporter
  template:
    metadata:
      labels:
        app: node-exporter
    spec:
      hostNetwork: true
      hostPID: true
      containers:
      - name: node-exporter
        image: prom/node-exporter:latest
        ports:
        - containerPort: 9100
          hostPort: 9100
        volumeMounts:
        - name: proc
          mountPath: /host/proc
          readOnly: true
        - name: sys
          mountPath: /host/sys
          readOnly: true
        - name: root
          mountPath: /rootfs
          readOnly: true
        args:
        - '--path.procfs=/host/proc'
        - '--path.sysfs=/host/sys'
        - '--collector.filesystem.ignored-mount-points'
        - '^/(sys|proc|dev|host|etc|rootfs/var/lib/docker/containers|rootfs/var/lib/docker/overlay2|rootfs/run/docker/netns|rootfs/var/lib/docker/aufs)($$|/)'
      volumes:
      - name: proc
        hostPath:
          path: /proc
      - name: sys
        hostPath:
          path: /sys
      - name: root
        hostPath:
          path: /
      tolerations:
      - effect: NoSchedule
        operator: Exists
```

#### 11.3.3 业务监控

**自定义指标配置**

```java
/**
 * 业务指标监控配置
 */
@Configuration
@EnableConfigurationProperties(MetricsProperties.class)
public class BusinessMetricsConfig {

    @Bean
    public MeterRegistryCustomizer<MeterRegistry> metricsCommonTags() {
        return registry -> registry.config().commonTags("application", "ddd-demo");
    }

    @Bean
    public TimedAspect timedAspect(MeterRegistry registry) {
        return new TimedAspect(registry);
    }

    @Bean
    public CountedAspect countedAspect(MeterRegistry registry) {
        return new CountedAspect(registry);
    }
}

/**
 * 业务指标收集器
 */
@Component
public class BusinessMetricsCollector {
    
    private final Counter userRegistrationCounter;
    private final Counter orderCreationCounter;
    private final Timer orderProcessingTimer;
    private final Gauge activeUsersGauge;
    
    public BusinessMetricsCollector(MeterRegistry meterRegistry) {
        this.userRegistrationCounter = Counter.builder("user.registration.total")
                .description("用户注册总数")
                .register(meterRegistry);
                
        this.orderCreationCounter = Counter.builder("order.creation.total")
                .description("订单创建总数")
                .tag("status", "success")
                .register(meterRegistry);
                
        this.orderProcessingTimer = Timer.builder("order.processing.duration")
                .description("订单处理耗时")
                .register(meterRegistry);
                
        this.activeUsersGauge = Gauge.builder("user.active.count")
                .description("活跃用户数")
                .register(meterRegistry, this, BusinessMetricsCollector::getActiveUserCount);
    }
    
    public void recordUserRegistration() {
        userRegistrationCounter.increment();
    }
    
    public void recordOrderCreation(String status) {
        orderCreationCounter.increment(Tags.of("status", status));
    }
    
    public Timer.Sample startOrderProcessing() {
        return Timer.start();
    }
    
    public void stopOrderProcessing(Timer.Sample sample) {
        sample.stop(orderProcessingTimer);
    }
    
    private double getActiveUserCount() {
        // 实现获取活跃用户数的逻辑
        return 0.0;
    }
}
```

### 11.4 日志管理

#### 11.4.1 日志收集

**Filebeat 配置**

```yaml
# logging/filebeat-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: filebeat-config
  namespace: logging
data:
  filebeat.yml: |
    filebeat.inputs:
    - type: container
      paths:
        - /var/log/containers/*ddd-demo*.log
      processors:
      - add_kubernetes_metadata:
          host: ${NODE_NAME}
          matchers:
          - logs_path:
              logs_path: "/var/log/containers/"
      - decode_json_fields:
          fields: ["message"]
          target: ""
          overwrite_keys: true
    
    output.elasticsearch:
      hosts: ["elasticsearch:9200"]
      index: "ddd-demo-logs-%{+yyyy.MM.dd}"
      template.name: "ddd-demo"
      template.pattern: "ddd-demo-*"
      template.settings:
        index.number_of_shards: 1
        index.number_of_replicas: 1
    
    logging.level: info
    logging.to_files: true
    logging.files:
      path: /var/log/filebeat
      name: filebeat
      keepfiles: 7
      permissions: 0644
```

**Logstash 配置**

```ruby
# logging/logstash-config.conf
input {
  beats {
    port => 5044
  }
}

filter {
  if [kubernetes][container][name] == "ddd-demo" {
    grok {
      match => { 
        "message" => "%{TIMESTAMP_ISO8601:timestamp} \[%{DATA:thread}\] %{LOGLEVEL:level} %{DATA:logger} - %{GREEDYDATA:log_message}"
      }
    }
    
    date {
      match => [ "timestamp", "yyyy-MM-dd HH:mm:ss" ]
    }
    
    if [log_message] =~ /^{.*}$/ {
      json {
        source => "log_message"
        target => "parsed_json"
      }
    }
    
    mutate {
      add_field => { "application" => "ddd-demo" }
      add_field => { "environment" => "%{[kubernetes][namespace]}" }
    }
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "ddd-demo-logs-%{+YYYY.MM.dd}"
  }
  
  if [level] == "ERROR" {
    email {
      to => "admin@example.com"
      subject => "DDD Demo Error Alert"
      body => "Error occurred: %{log_message}"
    }
  }
}
```

#### 11.4.2 日志分析

**Kibana Dashboard 配置**

```json
{
  "version": "7.15.0",
  "objects": [
    {
      "id": "ddd-demo-dashboard",
      "type": "dashboard",
      "attributes": {
        "title": "DDD Demo 应用监控",
        "hits": 0,
        "description": "DDD Demo 应用日志和指标监控面板",
        "panelsJSON": "[{\"version\":\"7.15.0\",\"gridData\":{\"x\":0,\"y\":0,\"w\":24,\"h\":15,\"i\":\"1\"},\"panelIndex\":\"1\",\"embeddableConfig\":{},\"panelRefName\":\"panel_1\"}]",
        "optionsJSON": "{\"useMargins\":true,\"syncColors\":false,\"hidePanelTitles\":false}",
        "version": 1,
        "timeRestore": false,
        "kibanaSavedObjectMeta": {
          "searchSourceJSON": "{\"query\":{\"query\":\"\",\"language\":\"kuery\"},\"filter\":[]}"
        }
      }
    }
  ]
}
```

#### 11.4.3 日志告警

**ElastAlert 配置**

```yaml
# logging/elastalert-rules.yaml
es_host: elasticsearch
es_port: 9200

rules_folder: /opt/elastalert/rules
run_every:
  minutes: 1

buffer_time:
  minutes: 15

writeback_index: elastalert_status

alert_time_limit:
  days: 2

---
# 错误日志告警规则
name: ddd-demo-error-alert
type: frequency
index: ddd-demo-logs-*
num_events: 5
timeframe:
  minutes: 5

filter:
- term:
    level: "ERROR"
- term:
    application: "ddd-demo"

alert:
- "email"
- "slack"

email:
- "admin@example.com"

slack:
webhook_url: "https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK"
slack_channel_override: "#alerts"
slack_username_override: "ElastAlert"

alert_text: |
  DDD Demo 应用出现错误日志
  时间: {0}
  错误数量: {1}
  详细信息: {2}

alert_text_args:
  - "@timestamp"
  - num_matches
  - log_message

---
# 性能异常告警规则
name: ddd-demo-performance-alert
type: metric_aggregation
index: ddd-demo-logs-*
metric_agg_key: response_time
metric_agg_type: avg
max_threshold: 2000

timeframe:
  minutes: 10

filter:
- exists:
    field: response_time
- term:
    application: "ddd-demo"

alert:
- "email"

email:
- "admin@example.com"

alert_text: |
  DDD Demo 应用响应时间异常
  平均响应时间: {0}ms
  阈值: 2000ms
```